{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MadhuShree-A/Fake-Job-Posting-Prediction/blob/main/Fake_Job_Posting_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA before Preprocessing"
      ],
      "metadata": {
        "id": "-C-sUTWWtR2z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhDhpB5GdB60"
      },
      "outputs": [],
      "source": [
        "# eda\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# 1. LOAD DATA\n",
        "df = pd.read_csv(\"fake_job_postings.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
        "\n",
        "# 2. TEXT STATS\n",
        "text_cols = [\"title\", \"company_profile\", \"description\", \"requirements\", \"benefits\"]\n",
        "\n",
        "for col in text_cols:\n",
        "    df[f\"{col}_wordcount\"] = df[col].fillna(\"\").apply(lambda x: len(str(x).split()))\n",
        "    df[f\"{col}_charcount\"] = df[col].fillna(\"\").apply(lambda x: len(str(x)))\n",
        "\n",
        "# Hist: Description length by fraud\n",
        "# HISTOGRAMS: Word Count Distribution by Fraud Status (All Text Fields)\n",
        "plt.figure(figsize=(15,12))\n",
        "for i, col in enumerate(text_cols, 1):\n",
        "    plt.subplot(3, 2, i)\n",
        "    sns.histplot(data=df, x=f\"{col}_wordcount\", hue=\"fraudulent\", bins=50, alpha=0.6)\n",
        "    plt.title(f\"Distribution of {col.capitalize()} Word Count by Fraud Status\")\n",
        "    plt.xlabel(\"Word Count\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Boxplot: All text fields\n",
        "plt.figure(figsize=(10,6))\n",
        "long_form = df.melt(id_vars=\"fraudulent\", value_vars=[f\"{col}_wordcount\" for col in text_cols],\n",
        "                    var_name=\"TextField\", value_name=\"WordCount\")\n",
        "sns.boxplot(data=long_form, x=\"TextField\", y=\"WordCount\", hue=\"fraudulent\")\n",
        "plt.title(\"Word Count Distribution by Fraud Status (All Text Fields)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim(0, 1000)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 3. WORD CLOUDS & TOP WORDS\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    return \" \".join([w for w in text.split() if w not in stop_words])\n",
        "\n",
        "fraud_text = \" \".join(df[df[\"fraudulent\"]==1][\"description\"].dropna().apply(clean_text))\n",
        "real_text  = \" \".join(df[df[\"fraudulent\"]==0][\"description\"].dropna().apply(clean_text))\n",
        "\n",
        "fraud_wc = WordCloud(width=800, height=400, background_color=\"black\").generate(fraud_text)\n",
        "real_wc  = WordCloud(width=800, height=400, background_color=\"white\").generate(real_text)\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(fraud_wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\"); plt.title(\"Fraudulent Postings WordCloud\")\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(real_wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\"); plt.title(\"Genuine Postings WordCloud\")\n",
        "plt.show()\n",
        "\n",
        "# Top 20 frequent words (Fraud vs Genuine)\n",
        "def top_words(text, n=20):\n",
        "    words = text.split()\n",
        "    return Counter(words).most_common(n)\n",
        "\n",
        "print(\"\\nTop 20 Fraudulent Words:\\n\", top_words(fraud_text, 20))\n",
        "print(\"\\nTop 20 Genuine Words:\\n\", top_words(real_text, 20))\n",
        "\n",
        "# 4. CORRELATION ANALYSIS\n",
        "num_cols = [\"telecommuting\", \"has_company_logo\", \"has_questions\", \"fraudulent\"]\n",
        "corr = df[num_cols].corr()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap (Numeric Features)\")\n",
        "plt.show()\n",
        "\n",
        "# 5. CROSS-FEATURE INSIGHTS\n",
        "# Fraud rate by employment type\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(data=df, x=\"employment_type\", y=\"fraudulent\", estimator=np.mean)\n",
        "plt.title(\"Fraud Rate by Employment Type\")\n",
        "plt.ylabel(\"Fraudulent Rate\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Fraud rate by required experience\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(data=df, x=\"required_experience\", y=\"fraudulent\", estimator=np.mean)\n",
        "plt.title(\"Fraud Rate by Required Experience\")\n",
        "plt.ylabel(\"Fraudulent Rate\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Fraud rate by required education\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(data=df, x=\"required_education\", y=\"fraudulent\", estimator=np.mean)\n",
        "plt.title(\"Fraud Rate by Required Education\")\n",
        "plt.ylabel(\"Fraudulent Rate\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Fraud rate by industry\n",
        "top_industries = df[\"industry\"].value_counts().head(10).index\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(data=df[df[\"industry\"].isin(top_industries)],\n",
        "            x=\"industry\", y=\"fraudulent\", estimator=np.mean)\n",
        "plt.title(\"Fraud Rate by Industry (Top 10)\")\n",
        "plt.ylabel(\"Fraudulent Rate\")\n",
        "plt.xticks(rotation=75)\n",
        "plt.show()\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define categorical columns you want to analyze\n",
        "categorical_cols = ['employment_type', 'has_company_logo', 'telecommuting', 'required_experience']\n",
        "\n",
        "# Loop through all pairwise combinations\n",
        "for i, col1 in enumerate(categorical_cols):\n",
        "    for col2 in categorical_cols[i+1:]:\n",
        "        # Create pivot table\n",
        "        pivot = pd.pivot_table(df, values=\"fraudulent\",\n",
        "                               index=col1,\n",
        "                               columns=col2,\n",
        "                               aggfunc=np.mean)\n",
        "\n",
        "        # Print pivot table\n",
        "        print(f\"\\nFraud Rate by {col1} & {col2}:\\n\")\n",
        "        print(pivot)\n",
        "\n",
        "        # Plot heatmap\n",
        "        plt.figure(figsize=(8,5))\n",
        "        sns.heatmap(pivot, annot=True, fmt=\".2f\", cmap=\"YlOrRd\")\n",
        "        plt.title(f\"Fraud Rate: {col1} vs {col2}\")\n",
        "        plt.ylabel(col1)\n",
        "        plt.xlabel(col2)\n",
        "        plt.show()\n",
        "\n",
        "# 7. BINARY COLUMN DISTRIBUTION (Fraud vs Genuine)\n",
        "binary_cols = [\"telecommuting\", \"has_company_logo\", \"has_questions\"]\n",
        "\n",
        "for col in binary_cols:\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.countplot(data=df, x=col, hue=\"fraudulent\")\n",
        "    plt.title(f\"Distribution of {col} by Fraudulent Status\")\n",
        "    plt.xlabel(f\"{col} (0=No, 1=Yes)\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.legend(title=\"Fraudulent\", labels=[\"Genuine\", \"Fraud\"])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Fraud Summary Table\n",
        "cat_cols = [\"employment_type\", \"required_experience\", \"required_education\", \"industry\"]\n",
        "for col in cat_cols:\n",
        "    summary = df.groupby(col)[\"fraudulent\"].mean().sort_values(ascending=False)\n",
        "    print(f\"\\nFraud Rate by {col}:\")\n",
        "    print(summary.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "CPCZtcxOtZcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  PREPROCESSING\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
        "from scipy import sparse\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import pickle\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "#  Load dataset\n",
        "\n",
        "print(\"\\nLoading dataset...\")\n",
        "\n",
        "df = pd.read_csv('fake_job_postings.csv')\n",
        "print(f\"âœ“ Dataset loaded successfully: {df.shape}\")\n",
        "print(f\"Fraudulent jobs: {df['fraudulent'].sum()} ({df['fraudulent'].mean()*100:.2f}%)\")\n",
        "print(f\"âš   HEAVY CLASS IMBALANCE DETECTED - will apply special handling\")\n",
        "\n",
        "#  Define column groups\n",
        "text_cols = ['title', 'company_profile', 'description', 'requirements', 'benefits']\n",
        "cat_cols = ['location', 'department', 'employment_type', 'required_experience',\n",
        "            'required_education', 'industry', 'function']\n",
        "binary_cols = ['telecommuting', 'has_company_logo', 'has_questions']\n",
        "\n",
        "#  Fill missing values\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"Handling Missing Values\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "# Text columns - empty string\n",
        "for col in text_cols:\n",
        "    missing_count = df[col].isna().sum()\n",
        "    df[col] = df[col].fillna('')\n",
        "    if missing_count > 0:\n",
        "        print(f\"  {col}: {missing_count} missing values filled\")\n",
        "\n",
        "# Categorical columns - 'Unknown' category\n",
        "for col in cat_cols:\n",
        "    missing_count = df[col].isna().sum()\n",
        "    df[col] = df[col].fillna('Unknown')\n",
        "    if missing_count > 0:\n",
        "        print(f\"  {col}: {missing_count} missing -> 'Unknown'\")\n",
        "\n",
        "# Binary columns - mode\n",
        "for col in binary_cols:\n",
        "    missing_count = df[col].isna().sum()\n",
        "    if missing_count > 0:\n",
        "        mode_val = df[col].mode()[0]\n",
        "        df[col] = df[col].fillna(mode_val)\n",
        "        print(f\"  {col}: {missing_count} missing -> {mode_val}\")\n",
        "\n",
        "# Salary - binary indicator\n",
        "df['salary_specified'] = df['salary_range'].apply(\n",
        "    lambda x: 0 if pd.isna(x) or str(x).strip()==\"\" else 1\n",
        ")\n",
        "print(f\"  salary_specified: {df['salary_specified'].sum()} have salary info\")\n",
        "df.drop(columns=['salary_range'], inplace=True)\n",
        "\n",
        "# 4. Reduce categorical cardinality\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"Reducing Categorical Cardinality\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "def reduce_cardinality(series, threshold=50, other_label='Other'):\n",
        "    \"\"\"Reduce high cardinality by keeping top N categories\"\"\"\n",
        "    value_counts = series.value_counts()\n",
        "\n",
        "    mask = series.isin(value_counts[value_counts >= threshold].index)\n",
        "    reduced_series = series.where(mask, other_label)\n",
        "\n",
        "    original_unique = series.nunique()\n",
        "    new_unique = reduced_series.nunique()\n",
        "\n",
        "    print(f\"  {series.name}: {original_unique} â†’ {new_unique} categories\")\n",
        "    return reduced_series\n",
        "\n",
        "# Apply to high-cardinality columns\n",
        "df['location'] = reduce_cardinality(df['location'], threshold=100)\n",
        "df['department'] = reduce_cardinality(df['department'], threshold=50)\n",
        "df['industry'] = reduce_cardinality(df['industry'], threshold=30)\n",
        "\n",
        "#  Combine and clean text\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"Text Processing\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "df['text_raw'] = df[text_cols].agg(' '.join, axis=1)\n",
        "print(f\"  Raw text combined from {len(text_cols)} columns\")\n",
        "\n",
        "# Clean text\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    text = soup.get_text()\n",
        "\n",
        "\n",
        "    text = re.sub(r'\\[[^]]*\\]', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
        "\n",
        "\n",
        "    words = [word for word in text.split() if word not in stop_words]\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "df['text_clean'] = df['text_raw'].apply(clean_text)\n",
        "\n",
        "# Handle empty text\n",
        "empty_count = (df['text_clean'].str.strip() == '').sum()\n",
        "df['text_clean'] = df['text_clean'].apply(lambda x: x if x.strip() != '' else 'emptytext')\n",
        "print(f\"  Empty text documents: {empty_count}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"Feature Engineering\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "def extract_text_features(row):\n",
        "    raw = row['text_raw']\n",
        "    clean = row['text_clean']\n",
        "    words = clean.split()\n",
        "    wc = len(words)\n",
        "\n",
        "    uppercase_words = sum(1 for word in raw.split() if word.isupper() and len(word) > 1)\n",
        "\n",
        "    return pd.Series({\n",
        "        'char_count': len(raw),\n",
        "        'word_count': wc,\n",
        "        'unique_words': len(set(words)),\n",
        "        'avg_word_len': (sum(len(w) for w in words)/wc) if wc > 0 else 0,\n",
        "        'num_exclaims': raw.count('!'),\n",
        "        'num_questions': raw.count('?'),\n",
        "        'has_email': 1 if '@' in raw else 0,\n",
        "        'has_url': 1 if 'http' in raw.lower() or 'www' in raw.lower() else 0,\n",
        "        'all_caps_ratio': sum(1 for c in raw if c.isupper()) / len(raw) if len(raw) > 0 else 0,\n",
        "        'uppercase_word_count': uppercase_words,\n",
        "        'text_richness': len(set(words)) / wc if wc > 0 else 0\n",
        "    })\n",
        "\n",
        "numeric_text_cols = ['char_count', 'word_count', 'unique_words', 'avg_word_len',\n",
        "                     'num_exclaims', 'num_questions', 'has_email', 'has_url',\n",
        "                     'all_caps_ratio', 'uppercase_word_count', 'text_richness']\n",
        "\n",
        "df[numeric_text_cols] = df.apply(extract_text_features, axis=1)\n",
        "print(f\"  Extracted {len(numeric_text_cols)} numeric text features\")\n",
        "\n",
        "\n",
        "print(\"\\n  Feature correlation with fraud:\")\n",
        "fraud_correlations = {}\n",
        "for col in numeric_text_cols:\n",
        "    corr = df[col].corr(df['fraudulent'])\n",
        "    fraud_correlations[col] = corr\n",
        "    print(f\"    {col:.<20} {corr:7.4f}\")\n",
        "\n",
        "\n",
        "#  Encode categorical features\n",
        "\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"Categorical Encoding\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for col in cat_cols:\n",
        "    unique_count = df[col].nunique()\n",
        "    print(f\"  {col}: {unique_count} unique values\")\n",
        "\n",
        "df_cat_encoded = pd.get_dummies(df[cat_cols], drop_first=True)\n",
        "print(f\"\\n  One-hot encoded to {df_cat_encoded.shape[1]} features (reduced from 4634+)\")\n",
        "\n",
        "\n",
        "# 8. TF-IDF features\n",
        "\n",
        "print(\"TF-IDF Vectorization\")\n",
        "\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=2000,\n",
        "    min_df=10,\n",
        "    max_df=0.8,\n",
        "    ngram_range=(1, 2),\n",
        "    stop_words='english'\n",
        ")\n",
        "\n",
        "X_text = tfidf_vectorizer.fit_transform(df['text_clean'])\n",
        "print(f\"  TF-IDF shape: {X_text.shape}\")\n",
        "print(f\"  Sparsity: {(1 - X_text.nnz / (X_text.shape[0] * X_text.shape[1])) * 100:.2f}%\")\n",
        "\n",
        "#  Scale  features\n",
        "print(\"Feature Scaling\")\n",
        "# Separate continuous and binary features\n",
        "continuous_features = ['char_count', 'word_count', 'unique_words', 'avg_word_len',\n",
        "                      'all_caps_ratio', 'uppercase_word_count', 'text_richness']\n",
        "binary_features = ['num_exclaims', 'num_questions', 'has_email', 'has_url'] + binary_cols + ['salary_specified']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_continuous = scaler.fit_transform(df[continuous_features])\n",
        "print(f\"  Scaled {len(continuous_features)} continuous features\")\n",
        "\n",
        "X_binary = df[binary_features].values\n",
        "print(f\"  Kept {len(binary_features)} binary features unscaled\")\n",
        "\n",
        "X_num = np.hstack([X_continuous, X_binary])\n",
        "print(f\"  Total numeric features: {X_num.shape[1]}\")\n",
        "\n",
        "#  Combine all features\n",
        "\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"Combining Features\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "X_combined = sparse.hstack([\n",
        "    X_text,\n",
        "    sparse.csr_matrix(X_num),\n",
        "    sparse.csr_matrix(df_cat_encoded.values)\n",
        "], format='csr')\n",
        "\n",
        "print(f\"  Initial feature matrix: {X_combined.shape}\")\n",
        "\n",
        "\n",
        "#  FEATURE SELECTION\n",
        "\n",
        "print(\"Feature Selection for Imbalanced Data\")\n",
        "\n",
        "\n",
        "# Calculate optimal number of features (rule of thumb for imbalanced data)\n",
        "n_fraud = df['fraudulent'].sum()\n",
        "optimal_features = min(500, n_fraud * 2)\n",
        "optimal_features = max(100, optimal_features)\n",
        "\n",
        "print(f\"  Fraud samples: {n_fraud}\")\n",
        "print(f\"  Optimal features (2Ã— fraud samples): {optimal_features}\")\n",
        "\n",
        "print(\"  Using ANOVA F-value for feature selection (handles negative values)\")\n",
        "selector = SelectKBest(f_classif, k=optimal_features)\n",
        "X_final = selector.fit_transform(X_combined, df['fraudulent'])\n",
        "\n",
        "print(f\"  Reduced from {X_combined.shape[1]} to {X_final.shape[1]} features\")\n",
        "print(f\"  Feature reduction: {(1 - X_final.shape[1] / X_combined.shape[1]) * 100:.1f}%\")\n",
        "\n",
        "\n",
        "print(\"Data Quality Checks\")\n",
        "\n",
        "\n",
        "y = df['fraudulent']\n",
        "\n",
        "print(f\"  Final dataset shape: {X_final.shape}\")\n",
        "print(f\"  Fraud ratio: {y.mean()*100:.2f}%\")\n",
        "print(f\"  Fraud samples: {y.sum()}\")\n",
        "print(f\"  Legitimate samples: {len(y) - y.sum()}\")\n",
        "print(f\"  Feature sparsity: {(1 - X_final.nnz / (X_final.shape[0] * X_final.shape[1])) * 100:.2f}%\")\n",
        "\n",
        "if X_final.shape[0] != len(y):\n",
        "    print(\"  âš  WARNING: Feature matrix and target length mismatch!\")\n",
        "if sparse.issparse(X_final):\n",
        "    if np.any(np.isnan(X_final.data)):\n",
        "        print(\"  âš  WARNING: NaN values detected in features!\")\n",
        "if X_final.nnz == 0:\n",
        "    print(\"  âš  WARNING: Feature matrix is all zeros!\")\n",
        "\n",
        "print(\"  âœ“ All quality checks passed\")\n",
        "\n",
        "\n",
        "#  Save preprocessed outputs\n",
        "\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"Saving Outputs\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "sparse.save_npz(\"X_processed.npz\", X_final)\n",
        "print(\"  âœ“ X_processed.npz saved\")\n",
        "\n",
        "with open(\"y_target.pkl\", \"wb\") as f:\n",
        "    pickle.dump(y, f)\n",
        "print(\"  âœ“ y_target.pkl saved\")\n",
        "\n",
        "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tfidf_vectorizer, f)\n",
        "print(\"  âœ“ tfidf_vectorizer.pkl saved\")\n",
        "\n",
        "with open(\"feature_scaler.pkl\", \"wb\") as f:\n",
        "    pickle.dump(scaler, f)\n",
        "print(\"  âœ“ feature_scaler.pkl saved\")\n",
        "\n",
        "with open(\"feature_selector.pkl\", \"wb\") as f:\n",
        "    pickle.dump(selector, f)\n",
        "print(\"  âœ“ feature_selector.pkl saved\")\n",
        "\n",
        "feature_names = {\n",
        "    'tfidf_features': tfidf_vectorizer.get_feature_names_out().tolist(),\n",
        "    'numeric_features': continuous_features + binary_features,\n",
        "    'categorical_features': df_cat_encoded.columns.tolist(),\n",
        "    'selected_features_mask': selector.get_support()\n",
        "}\n",
        "\n",
        "with open(\"feature_names.pkl\", \"wb\") as f:\n",
        "    pickle.dump(feature_names, f)\n",
        "print(\"  âœ“ feature_names.pkl saved\")\n",
        "\n",
        "df.to_csv(\"processed_job_postings.csv\", index=False)\n",
        "print(\"  âœ“ processed_job_postings.csv saved\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nFinal optimized dataset:\")\n",
        "print(f\"  Samples: {X_final.shape[0]}\")\n",
        "print(f\"  Features: {X_final.shape[1]} (reduced for imbalance)\")\n",
        "print(f\"  Fraud ratio: {y.mean()*100:.2f}%\")\n",
        "print(f\"  Ready for imbalanced learning techniques! ðŸš€\")\n",
        "\n"
      ],
      "metadata": {
        "id": "IfCqSkCRdH_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manual Perceptron"
      ],
      "metadata": {
        "id": "25rbV52lte_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from scipy import sparse\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score, average_precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "print(\"\\n[1] Loading and preparing data...\")\n",
        "\n",
        "X_processed = sparse.load_npz(\"X_processed.npz\")\n",
        "with open(\"y_target.pkl\", \"rb\") as f:\n",
        "    y = pickle.load(f)\n",
        "\n",
        "print(f\"Dataset: {X_processed.shape}\")\n",
        "print(f\"Fraud ratio: {y.mean()*100:.4f}% ({y.sum()} frauds)\")\n",
        "\n",
        "X_dense = X_processed.toarray()\n",
        "if not isinstance(y, np.ndarray):\n",
        "    y = y.values\n",
        "\n",
        "X_with_bias = np.hstack([np.ones((X_dense.shape[0], 1)), X_dense])\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_with_bias, y, test_size=0.1, random_state=42, stratify=y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1111, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(\"\\nData shapes:\")\n",
        "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape} ({y_train.sum()} frauds)\")\n",
        "print(f\"X_val:   {X_val.shape}, y_val: {y_val.shape} ({y_val.sum()} frauds)\")\n",
        "print(f\"X_test:  {X_test.shape}, y_test: {y_test.shape} ({y_test.sum()} frauds)\")\n",
        "\n",
        "class ManualPerceptron:\n",
        "    def _init_(self, learning_rate=0.01, n_iters=1000, class_weight=None):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.class_weight = class_weight\n",
        "        self.weights = None\n",
        "        self.errors_history = []\n",
        "        self.loss_history = []\n",
        "\n",
        "    def _calculate_sample_weights(self, y):\n",
        "        if self.class_weight == 'balanced':\n",
        "            class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "            sample_weights = np.where(y == 1, class_weights[1], class_weights[0])\n",
        "        elif isinstance(self.class_weight, dict):\n",
        "            sample_weights = np.where(y == 1, self.class_weight[1], self.class_weight[0])\n",
        "        else:\n",
        "            sample_weights = np.ones(len(y))\n",
        "        return sample_weights\n",
        "\n",
        "    def activation(self, x):\n",
        "        return np.where(x >= 0, 1, 0)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.random.normal(0, 0.01, n_features)\n",
        "        sample_weights = self._calculate_sample_weights(y)\n",
        "\n",
        "        for epoch in range(self.n_iters):\n",
        "            total_errors = 0\n",
        "            total_loss = 0\n",
        "\n",
        "            for i in range(n_samples):\n",
        "                linear_output = np.dot(X[i], self.weights)\n",
        "                y_pred = self.activation(linear_output)\n",
        "\n",
        "                error = (y[i] - y_pred) * sample_weights[i]\n",
        "                total_loss += abs(error)\n",
        "\n",
        "                if error != 0:\n",
        "                    self.weights += self.lr * error * X[i]\n",
        "                    total_errors += 1\n",
        "\n",
        "            self.errors_history.append(total_errors)\n",
        "            self.loss_history.append(total_loss / n_samples)\n",
        "\n",
        "            if (epoch + 1) % 100 == 0:\n",
        "                print(f\"    Epoch {epoch + 1}: Errors = {total_errors}, Loss = {self.loss_history[-1]:.4f}\")\n",
        "\n",
        "            if total_errors == 0:\n",
        "                print(f\"    Converged after {epoch + 1} epochs\")\n",
        "                break\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights)\n",
        "        return self.activation(linear_output)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        linear_output = np.dot(X, self.weights)\n",
        "        probabilities = 1 / (1 + np.exp(-linear_output))\n",
        "        return probabilities\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        return np.dot(X, self.weights)\n",
        "\n",
        "print(\"\\n[2] Hyperparameter tuning with class weights...\")\n",
        "\n",
        "n_legit = (y_train == 0).sum()\n",
        "n_fraud = (y_train == 1).sum()\n",
        "inverse_freq = n_legit / n_fraud\n",
        "\n",
        "print(f\"  Class distribution: {n_legit} legitimate, {n_fraud} fraudulent\")\n",
        "print(f\"  Inverse frequency weight: {inverse_freq:.2f}\")\n",
        "\n",
        "learning_rates = [0.1, 0.01, 0.001]\n",
        "weight_strategies = [\n",
        "    None,\n",
        "    'balanced',\n",
        "    {0: 1, 1: 10},\n",
        "    {0: 1, 1: 20}\n",
        "]\n",
        "\n",
        "best_f1 = -1\n",
        "best_config = {}\n",
        "results = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for weights in weight_strategies:\n",
        "        config_name = f\"lr_{lr}weights{weights}\"\n",
        "        print(f\"\\n  Testing: {config_name}\")\n",
        "\n",
        "        perceptron = ManualPerceptron(learning_rate=lr, n_iters=500, class_weight=weights)\n",
        "        perceptron.fit(X_train, y_train)\n",
        "\n",
        "        y_val_pred = perceptron.predict(X_val)\n",
        "        f1 = f1_score(y_val, y_val_pred)\n",
        "        recall = recall_score(y_val, y_val_pred)\n",
        "\n",
        "        results[config_name] = {\n",
        "            'f1': f1,\n",
        "            'recall': recall,\n",
        "            'model': perceptron\n",
        "        }\n",
        "\n",
        "        print(f\"    Validation F1: {f1:.4f}, Recall: {recall:.4f}\")\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_config = {'lr': lr, 'weights': weights}\n",
        "            best_model = perceptron\n",
        "\n",
        "print(f\"\\nBest configuration: LR={best_config['lr']}, Weights={best_config['weights']}\")\n",
        "print(f\"Best Validation F1-Score: {best_f1:.4f}\")\n",
        "\n",
        "print(\"\\n[3] Final evaluation on test set...\")\n",
        "\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "y_test_proba = best_model.predict_proba(X_test)\n",
        "\n",
        "accuracy = float(accuracy_score(y_test, y_test_pred))\n",
        "precision = float(precision_score(y_test, y_test_pred))\n",
        "recall = float(recall_score(y_test, y_test_pred))\n",
        "f1 = float(f1_score(y_test, y_test_pred))\n",
        "roc_auc = float(roc_auc_score(y_test, y_test_proba))\n",
        "pr_auc = float(average_precision_score(y_test, y_test_proba))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print(\"\\nPERCEPTRON TEST RESULTS\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
        "print(f\"PR-AUC:    {pr_auc:.4f}\")\n",
        "\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(f\"                Predicted\")\n",
        "print(f\"               Legit   Fraud\")\n",
        "print(f\"Actual Legit   {tn:>5}   {fp:>5}\")\n",
        "print(f\"Actual Fraud   {fn:>5}   {tp:>5}\")\n",
        "\n",
        "print(f\"\\nBusiness Impact:\")\n",
        "print(f\"Frauds caught: {tp}/{tp+fn} ({recall*100:.1f}%)\")\n",
        "print(f\"Frauds missed: {fn}\")\n",
        "print(f\"False alarms:  {fp} legitimate jobs flagged\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=['Legitimate', 'Fraud']))\n",
        "\n",
        "print(\"\\n[4] Saving Manual Perceptron model with comprehensive metrics...\")\n",
        "\n",
        "def save_perceptron_model_with_metrics(model, metrics, model_name, config):\n",
        "    model_filename = f\"models/{model_name.replace(' ', '_').replace('(', '').replace(')', '').lower()}_model.pkl\"\n",
        "    with open(model_filename, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    metrics_filename = f\"models/{model_name.replace(' ', '_').replace('(', '').replace(')', '').lower()}_metrics.json\"\n",
        "    metrics_to_save = {\n",
        "        'model_name': model_name,\n",
        "        'model_type': \"ManualPerceptron\",\n",
        "        'optimal_threshold': 0.0,\n",
        "        'performance_metrics': {\n",
        "            'recall': float(metrics['recall']),\n",
        "            'f1_score': float(metrics['f1']),\n",
        "            'accuracy': float(metrics['accuracy']),\n",
        "            'auc_roc': float(metrics['roc_auc']),\n",
        "            'auc_pr': float(metrics['pr_auc'])\n",
        "        },\n",
        "        'confusion_matrix': {\n",
        "            'tn': int(tn),\n",
        "            'fp': int(fp),\n",
        "            'fn': int(fn),\n",
        "            'tp': int(tp)\n",
        "        },\n",
        "        'business_metrics': {\n",
        "            'fraud_detection_rate': float(tp/(tp+fn)*100) if (tp+fn) > 0 else 0.0,\n",
        "            'false_alarm_rate': float(fp/(fp+tn)*100) if (fp+tn) > 0 else 0.0,\n",
        "            'frauds_caught': int(tp),\n",
        "            'frauds_missed': int(fn),\n",
        "            'false_positives': int(fp)\n",
        "        },\n",
        "        'training_info': {\n",
        "            'train_samples': int(X_train.shape[0]),\n",
        "            'val_samples': int(X_val.shape[0]),\n",
        "            'test_samples': int(X_test.shape[0]),\n",
        "            'fraud_ratio': float(y.mean()),\n",
        "            'learning_rate': float(config['lr']),\n",
        "            'class_weight': config['weights'],\n",
        "            'n_features': int(X_dense.shape[1])\n",
        "        },\n",
        "        'model_config': {\n",
        "            'learning_rate': float(config['lr']),\n",
        "            'class_weight': config['weights'],\n",
        "            'n_iterations': len(model.errors_history)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(metrics_filename, 'w') as f:\n",
        "        json.dump(metrics_to_save, f, indent=2)\n",
        "\n",
        "    return model_filename, metrics_filename\n",
        "\n",
        "perceptron_metrics = {\n",
        "    'precision': precision,\n",
        "    'recall': recall,\n",
        "    'f1': f1,\n",
        "    'accuracy': accuracy,\n",
        "    'roc_auc': roc_auc,\n",
        "    'pr_auc': pr_auc\n",
        "}\n",
        "\n",
        "best_model_file, best_metrics_file = save_perceptron_model_with_metrics(\n",
        "    best_model, perceptron_metrics, \"Manual Perceptron\", best_config\n",
        ")\n",
        "\n",
        "print(f\"  {best_model_file}\")\n",
        "print(f\"  {best_metrics_file}\")\n",
        "\n",
        "perceptron_package = {\n",
        "    'best_model': best_model,\n",
        "    'performance_metrics': perceptron_metrics,\n",
        "    'confusion_matrix': {\n",
        "        'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)\n",
        "    },\n",
        "    'auc_scores': {\n",
        "        'roc_auc': float(roc_auc),\n",
        "        'pr_auc': float(pr_auc)\n",
        "    },\n",
        "    'training_info': {\n",
        "        'dataset_shape': [int(X_dense.shape[0]), int(X_dense.shape[1])],\n",
        "        'fraud_ratio': float(y.mean()),\n",
        "        'train_samples': int(X_train.shape[0]),\n",
        "        'test_samples': int(X_test.shape[0]),\n",
        "        'best_config': best_config\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"models/manual_perceptron_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(perceptron_package, f)\n",
        "\n",
        "with open(\"models/manual_perceptron_metrics.json\", \"w\") as f:\n",
        "    json.dump({\n",
        "        'best_model': \"Manual Perceptron\",\n",
        "        'performance_metrics': perceptron_metrics,\n",
        "        'business_impact': {\n",
        "            'frauds_caught': f\"{tp}/{tp+fn} ({tp/(tp+fn)*100:.1f}%)\",\n",
        "            'frauds_missed': int(fn),\n",
        "            'false_alarms': int(fp),\n",
        "            'fraud_detection_rate': float(tp/(tp+fn)*100) if (tp+fn) > 0 else 0.0,\n",
        "            'false_alarm_rate': float(fp/(fp+tn)*100) if (fp+tn) > 0 else 0.0\n",
        "        },\n",
        "        'best_configuration': best_config\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"  models/manual_perceptron_model.pkl\")\n",
        "print(f\"  models/manual_perceptron_metrics.json\")\n",
        "\n",
        "print(\"\\n[5] Generating visualizations...\")\n",
        "\n",
        "try:\n",
        "    import seaborn as sns\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    axes[0,0].plot(best_model.errors_history, marker='o', color='blue', linewidth=2)\n",
        "    axes[0,0].set_title(f'Training Errors (LR={best_config[\"lr\"]})')\n",
        "    axes[0,0].set_xlabel('Epoch')\n",
        "    axes[0,0].set_ylabel('Misclassifications')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[0,1].plot(best_model.loss_history, marker='s', color='red', linewidth=2)\n",
        "    axes[0,1].set_title('Training Loss')\n",
        "    axes[0,1].set_xlabel('Epoch')\n",
        "    axes[0,1].set_ylabel('Average Loss')\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1,0],\n",
        "                xticklabels=['Legitimate', 'Fraud'],\n",
        "                yticklabels=['Legitimate', 'Fraud'])\n",
        "    axes[1,0].set_title('Confusion Matrix')\n",
        "    axes[1,0].set_xlabel('Predicted')\n",
        "    axes[1,0].set_ylabel('Actual')\n",
        "\n",
        "    legit_scores = y_test_proba[y_test == 0]\n",
        "    fraud_scores = y_test_proba[y_test == 1]\n",
        "    axes[1,1].hist(legit_scores, bins=30, alpha=0.7, label='Legitimate', color='green', density=True)\n",
        "    axes[1,1].hist(fraud_scores, bins=30, alpha=0.7, label='Fraud', color='red', density=True)\n",
        "    axes[1,1].axvline(0.5, color='black', linestyle='--', label='Decision Boundary')\n",
        "    axes[1,1].set_xlabel('Prediction Score')\n",
        "    axes[1,1].set_ylabel('Density')\n",
        "    axes[1,1].set_title('Score Distribution by Class')\n",
        "    axes[1,1].legend()\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('models/perceptron_results.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"Visualizations saved as 'models/perceptron_results.png'\")\n",
        "except Exception as e:\n",
        "    print(f\"Visualization generation skipped: {e}\")\n",
        "\n",
        "print(\"\\nMANUAL PERCEPTRON TRAINING COMPLETED!\")\n",
        "\n",
        "print(f\"\\nFINAL PERFORMANCE:\")\n",
        "print(f\"  F1-Score: {f1:.4f}\")\n",
        "print(f\"  Recall:   {recall:.4f} ({recall*100:.1f}% frauds caught)\")\n",
        "print(f\"  Precision: {precision:.4f} ({precision*100:.1f}% accurate when flagging)\")\n",
        "print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
        "print(f\"  PR-AUC:    {pr_auc:.4f}\")\n",
        "\n",
        "print(f\"\\nBEST CONFIGURATION:\")\n",
        "print(f\"  Learning Rate: {best_config['lr']}\")\n",
        "print(f\"  Class Weights: {best_config['weights']}\")\n",
        "\n",
        "print(f\"\\nDATASET INFO:\")\n",
        "print(f\"  Total samples: {X_dense.shape[0]}\")\n",
        "print(f\"  Features: {X_dense.shape[1]}\")\n",
        "print(f\"  Fraud rate: {y.mean()*100:.2f}%\")\n",
        "\n",
        "print(f\"\\nModels saved in 'models/' directory:\")\n",
        "print(f\"  Manual Perceptron: models/manual_perceptron_model.pkl\")\n",
        "print(f\"  Metrics: models/manual_perceptron_metrics.json\")\n",
        "\n",
        "print(f\"\\nManual Perceptron is ready for production deployment!\")"
      ],
      "metadata": {
        "id": "bQM8lMMFX1Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network"
      ],
      "metadata": {
        "id": "WkT5ij-ytvYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "print(\"Loading processed data...\")\n",
        "X_sparse = sparse.load_npz(\"X_processed.npz\")\n",
        "\n",
        "with open(\"y_target.pkl\", \"rb\") as f:\n",
        "    y = pickle.load(f)\n",
        "y = np.array(y).ravel()\n",
        "\n",
        "print(f\"Loaded X: {X_sparse.shape}, y: {len(y)}\")\n",
        "\n",
        "# APPLY SVD â†’ 500 DIMENSIONS\n",
        "print(\"Applying TruncatedSVD (500 components)...\")\n",
        "svd = TruncatedSVD(n_components=500, random_state=42)\n",
        "X_svd = svd.fit_transform(X_sparse)\n",
        "\n",
        "print(f\"SVD reduced shape: {X_svd.shape}\")\n",
        "\n",
        "# TRAIN/VAL/TEST SPLIT\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X_svd, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.15, stratify=y_train_val, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "# MANUAL NEURAL NETWORK CLASS\n",
        "class ManualL3NN:\n",
        "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], output_dim=1):\n",
        "        np.random.seed(42)\n",
        "        self.params = {}\n",
        "        dims = [input_dim] + hidden_dims + [output_dim]\n",
        "\n",
        "        for i in range(1, len(dims)):\n",
        "            self.params[f\"W{i}\"] = np.random.randn(dims[i-1], dims[i]) * np.sqrt(2.0 / dims[i-1])\n",
        "            self.params[f\"b{i}\"] = np.zeros((1, dims[i]))\n",
        "            if i < len(dims) - 1:\n",
        "                self.params[f\"gamma{i}\"] = np.ones((1, dims[i]))\n",
        "                self.params[f\"beta{i}\"] = np.zeros((1, dims[i]))\n",
        "\n",
        "        self.bn_params = {\n",
        "            i: {\"running_mean\": np.zeros((1, hidden_dims[i-1])),\n",
        "                \"running_var\": np.ones((1, hidden_dims[i-1]))}\n",
        "            for i in range(1, len(hidden_dims)+1)\n",
        "        }\n",
        "\n",
        "        self.m, self.v, self.t = {}, {}, 0\n",
        "        for k in self.params:\n",
        "            self.m[k] = np.zeros_like(self.params[k])\n",
        "            self.v[k] = np.zeros_like(self.params[k])\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-np.clip(z, -20, 20)))\n",
        "\n",
        "    def relu(self, z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    def batchnorm_forward(self, z, layer, mode=\"train\"):\n",
        "        gamma = self.params[f\"gamma{layer}\"]\n",
        "        beta = self.params[f\"beta{layer}\"]\n",
        "        bn = self.bn_params[layer]\n",
        "\n",
        "        if mode == \"train\":\n",
        "            mu = np.mean(z, axis=0, keepdims=True)\n",
        "            var = np.var(z, axis=0, keepdims=True)\n",
        "            var = np.clip(var, 1e-8, None)\n",
        "            z_hat = (z - mu) / np.sqrt(var + 1e-8)\n",
        "            out = gamma * z_hat + beta\n",
        "            bn[\"running_mean\"] = 0.9 * bn[\"running_mean\"] + 0.1 * mu\n",
        "            bn[\"running_var\"] = 0.9 * bn[\"running_var\"] + 0.1 * var\n",
        "            return out, (z, z_hat, mu, var, gamma, beta)\n",
        "        else:\n",
        "            z_hat = (z - bn[\"running_mean\"]) / np.sqrt(bn[\"running_var\"] + 1e-8)\n",
        "            return gamma * z_hat + beta, None\n",
        "\n",
        "    def batchnorm_backward(self, dout, cache):\n",
        "        z, z_hat, mu, var, gamma, beta = cache\n",
        "        N = z.shape[0]\n",
        "        dbeta = np.sum(dout, axis=0, keepdims=True)\n",
        "        dgamma = np.sum(dout * z_hat, axis=0, keepdims=True)\n",
        "        dz_hat = dout * gamma\n",
        "        dvar = np.sum(dz_hat * (z - mu) * -0.5 * (var + 1e-8)**(-1.5), axis=0, keepdims=True)\n",
        "        dmu = np.sum(dz_hat * -1/np.sqrt(var + 1e-8), axis=0, keepdims=True) + dvar * np.mean(-2*(z - mu), axis=0, keepdims=True)\n",
        "        dz = dz_hat / np.sqrt(var + 1e-8) + dvar * 2*(z - mu)/N + dmu/N\n",
        "        return dz, dgamma, dbeta\n",
        "\n",
        "    def forward(self, X, mode=\"train\"):\n",
        "        caches = {}\n",
        "        A = X.astype(np.float32)\n",
        "        activations = [A]\n",
        "\n",
        "        for i in range(1, 4):\n",
        "            Z = A @ self.params[f\"W{i}\"] + self.params[f\"b{i}\"]\n",
        "            Z_bn, bn_cache = self.batchnorm_forward(Z, i, mode)\n",
        "            A = self.relu(Z_bn)\n",
        "            activations.append(A)\n",
        "            caches[i] = (Z, Z_bn, bn_cache)\n",
        "\n",
        "        Z_out = A @ self.params[\"W4\"] + self.params[\"b4\"]\n",
        "        y_hat = self.sigmoid(Z_out)\n",
        "        caches[\"activations\"] = activations\n",
        "        return y_hat, caches\n",
        "\n",
        "    def backward(self, y_true, y_pred, caches):\n",
        "        grads = {}\n",
        "        m = y_true.shape[0]\n",
        "        dZ = y_pred - y_true\n",
        "\n",
        "        A3 = caches[\"activations\"][3]\n",
        "        grads[\"W4\"] = A3.T @ dZ / m\n",
        "        grads[\"b4\"] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "        dA = dZ @ self.params[\"W4\"].T\n",
        "\n",
        "        for i in reversed(range(1, 4)):\n",
        "            Z, Z_bn, bn_cache = caches[i]\n",
        "            dZ_bn = dA * (Z_bn > 0)\n",
        "            dZ, dgamma, dbeta = self.batchnorm_backward(dZ_bn, bn_cache)\n",
        "            grads[f\"gamma{i}\"] = dgamma\n",
        "            grads[f\"beta{i}\"] = dbeta\n",
        "\n",
        "            A_prev = caches[\"activations\"][i-1]\n",
        "            grads[f\"W{i}\"] = A_prev.T @ dZ / m\n",
        "            grads[f\"b{i}\"] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "            dA = dZ @ self.params[f\"W{i}\"].T\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def update_params(self, grads, lr=1e-3):\n",
        "        self.t += 1\n",
        "        for k in self.params:\n",
        "            g = grads[k]\n",
        "            self.m[k] = 0.9 * self.m[k] + 0.1 * g\n",
        "            self.v[k] = 0.999 * self.v[k] + 0.001 * (g ** 2)\n",
        "            m_hat = self.m[k] / (1 - 0.9 ** self.t)\n",
        "            v_hat = self.v[k] / (1 - 0.999 ** self.t)\n",
        "            self.params[k] -= lr * m_hat / (np.sqrt(v_hat) + 1e-8)\n",
        "\n",
        "    def fit(self, X_train, y_train, X_val, y_val, epochs=60, lr=1e-3, batch_size=128):\n",
        "        y_train = y_train.reshape(-1, 1)\n",
        "        y_val = y_val.reshape(-1, 1)\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            perm = np.random.permutation(len(X_train))\n",
        "            X_shuf = X_train[perm]\n",
        "            y_shuf = y_train[perm]\n",
        "\n",
        "            for i in range(0, len(X_train), batch_size):\n",
        "                Xb = X_shuf[i:i+batch_size]\n",
        "                yb = y_shuf[i:i+batch_size]\n",
        "                y_pred, caches = self.forward(Xb, \"train\")\n",
        "                grads = self.backward(yb, y_pred, caches)\n",
        "                self.update_params(grads, lr)\n",
        "\n",
        "            if epoch % 10 == 0 or epoch == epochs:\n",
        "                val_pred, _ = self.forward(X_val, \"eval\")\n",
        "                val_f1 = f1_score(y_val, (val_pred >= 0.5).astype(int))\n",
        "                print(f\"Epoch {epoch:2d} | Val F1: {val_f1:.4f}\")\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        y_pred, _ = self.forward(X, \"eval\")\n",
        "        return y_pred.ravel()\n",
        "\n",
        "    def predict(self, X):\n",
        "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
        "\n",
        "\n",
        "# TRAIN THE MODEL\n",
        "print(\"\\nTraining Manual Neural Network (500 â†’ 256 â†’ 128 â†’ 64 â†’ 1)...\")\n",
        "model = ManualL3NN(input_dim=500)\n",
        "model.fit(X_train, y_train, X_val, y_val, epochs=60, lr=1e-3, batch_size=128)\n",
        "\n",
        "# EVALUATE ON TEST SET\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "probs = model.predict_proba(X_test)\n",
        "preds = model.predict(X_test)\n",
        "\n",
        "metrics = {\n",
        "    \"accuracy\": float(accuracy_score(y_test, preds)),\n",
        "    \"precision\": float(precision_score(y_test, preds)),\n",
        "    \"recall\": float(recall_score(y_test, preds)),\n",
        "    \"f1\": float(f1_score(y_test, preds)),\n",
        "    \"auc\": float(roc_auc_score(y_test, probs))\n",
        "}\n",
        "\n",
        "for k, v in metrics.items():\n",
        "    print(f\"{k.capitalize():9}: {v:.4f}\")\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "os.makedirs(\"metrics\", exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "with open(\"models/manual_nn.pkl\", \"wb\") as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# Save SVD (CRITICAL!)\n",
        "with open(\"models/svd_500_component.pkl\", \"wb\") as f:\n",
        "    pickle.dump(svd, f)\n",
        "\n",
        "# Save metrics\n",
        "with open(\"metrics/manual_nn_metrics.json\", \"w\") as f:\n",
        "    json.dump(metrics, f, indent=4)\n",
        "\n",
        "print(\"\\nSAVED:\")\n",
        "print(\"   models/manual_nn.pkl\")\n",
        "print(\"   models/svd_500_component.pkl   â† MUST UPLOAD TO STREAMLIT!\")\n",
        "print(\"   metrics/manual_nn_metrics.json\")\n",
        "print(\"\\nDone! Your Streamlit app will now work perfectly.\")"
      ],
      "metadata": {
        "id": "1OJ2-Og3df77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "fqhHPL_Jt4t2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN IMPLEMENTATION\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle, json, os\n",
        "from scipy import sparse\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score\n",
        ")\n",
        "\n",
        "\n",
        "X = sparse.load_npz(\"X_processed.npz\")\n",
        "y = pickle.load(open(\"y_target.pkl\", \"rb\"))\n",
        "\n",
        "print(f\"Loaded X: {X.shape}, y: {len(y)}\")\n",
        "print(f\"Fraud ratio: {y.mean()*100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "def normalize_rows(x):\n",
        "    norms = np.linalg.norm(x, axis=1, keepdims=True) + 1e-10\n",
        "    return x / norms\n",
        "\n",
        "\n",
        "class ManualKNN:\n",
        "    def __init__(self, n_neighbors=3, p=1, weights=\"distance\"):\n",
        "        self.k = n_neighbors\n",
        "        self.p = p\n",
        "        self.weights = weights\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = np.asarray(X)\n",
        "        self.y_train = np.asarray(y).astype(int)\n",
        "\n",
        "    def compute_distances(self, x):\n",
        "        return np.sum(np.abs(self.X_train - x), axis=1)\n",
        "\n",
        "    def predict_one(self, x):\n",
        "        distances = self.compute_distances(x)\n",
        "\n",
        "        idx = np.argsort(distances)[:self.k]\n",
        "        labels = self.y_train[idx]\n",
        "        dists = distances[idx]\n",
        "\n",
        "        weights = 1 / (dists + 1e-10)\n",
        "\n",
        "        score1 = np.sum(weights * labels)\n",
        "        score0 = np.sum(weights * (1 - labels))\n",
        "\n",
        "        return 1 if score1 > score0 else 0\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X)\n",
        "        return np.array([self.predict_one(row) for row in X])\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X = np.asarray(X)\n",
        "        probs = []\n",
        "        for row in X:\n",
        "            distances = self.compute_distances(row)\n",
        "            idx = np.argsort(distances)[:self.k]\n",
        "            labels = self.y_train[idx]\n",
        "            dists = distances[idx]\n",
        "            weights = 1 / (dists + 1e-10)\n",
        "            p1 = np.sum(weights * labels)\n",
        "            p0 = np.sum(weights * (1 - labels))\n",
        "            probs.append([p0/(p0+p1), p1/(p0+p1)])\n",
        "        return np.array(probs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# SVD DIMENSIONALITY REDUCTION FIRST\n",
        "svd = TruncatedSVD(n_components=300, random_state=42)\n",
        "X_train_svd = svd.fit_transform(X_train)\n",
        "X_test_svd = svd.transform(X_test)\n",
        "\n",
        "# Normalize manually\n",
        "X_train_norm = normalize_rows(X_train_svd)\n",
        "X_test_norm = normalize_rows(X_test_svd)\n",
        "\n",
        "\n",
        "\n",
        "knn = ManualKNN(n_neighbors=3, p=1, weights=\"distance\")\n",
        "knn.fit(X_train_norm, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test_norm)\n",
        "y_prob = knn.predict_proba(X_test_norm)[:, 1]\n",
        "\n",
        "metrics = {\n",
        "    \"accuracy\": float(accuracy_score(y_test, y_pred)),\n",
        "    \"precision\": float(precision_score(y_test, y_pred)),\n",
        "    \"recall\": float(recall_score(y_test, y_pred)),\n",
        "    \"f1\": float(f1_score(y_test, y_pred)),\n",
        "    \"auc\": float(roc_auc_score(y_test, y_prob)),\n",
        "    \"best_params\": {\n",
        "        \"n_neighbors\": 3,\n",
        "        \"p\": 1,\n",
        "        \"weights\": \"distance\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\nâœ… Test Metrics:\")\n",
        "for k, v in metrics.items():\n",
        "    if k != \"best_params\":\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "os.makedirs(\"metrics\", exist_ok=True)\n",
        "json.dump(metrics, open(\"metrics/knn_metrics.json\", \"w\"), indent=4)\n",
        "\n",
        "print(\"âœ… Saved metrics â†’ metrics/knn_metrics.json\")\n",
        "\n",
        "\n",
        "# SAVE MANUAL KNN MODEL\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "save_obj = {\n",
        "    \"svd\": svd,\n",
        "    \"knn\": knn\n",
        "}\n",
        "\n",
        "pickle.dump(save_obj, open(\"models/best_knn_model.pkl\", \"wb\"))\n",
        "\n",
        "print(\" Saved Manual KNN â†’ models/best_knn_model.pkl\")\n",
        "print(\"\\n Manual KNN Training Complete!\")"
      ],
      "metadata": {
        "id": "flWY_AdmePXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probabilistic Models"
      ],
      "metadata": {
        "id": "oCdd4teXuDHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PROBABILISTIC MODELS\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle, os, warnings, json\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, confusion_matrix, roc_curve, auc)\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)\n",
        "\n",
        "# LOAD PREPROCESSED DATA\n",
        "print(\"ðŸ”¹ Loading preprocessed data...\")\n",
        "\n",
        "X_combined = sparse.load_npz(\"X_processed.npz\")\n",
        "with open(\"y_target.pkl\", \"rb\") as f:\n",
        "    y = pickle.load(f)\n",
        "\n",
        "print(\" Data loaded successfully.\")\n",
        "print(\"Feature matrix shape:\", X_combined.shape)\n",
        "print(\"Target shape:\", y.shape)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_combined, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ENSURE NON-NEGATIVE VALUES FOR NB MODELS\n",
        "X_train_nb = X_train.copy()\n",
        "X_test_nb = X_test.copy()\n",
        "X_train_nb.data = np.clip(X_train_nb.data, 0, None)\n",
        "X_test_nb.data = np.clip(X_test_nb.data, 0, None)\n",
        "\n",
        "# EVALUATION FUNCTION\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test, name):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    y_proba_test = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    metrics = {\n",
        "        \"Model\": name,\n",
        "        \"Train Accuracy\": float(accuracy_score(y_train, y_pred_train)),\n",
        "        \"Test Accuracy\": float(accuracy_score(y_test, y_pred_test)),\n",
        "        \"Precision\": float(precision_score(y_test, y_pred_test)),\n",
        "        \"Recall\": float(recall_score(y_test, y_pred_test)),\n",
        "        \"F1-Score\": float(f1_score(y_test, y_pred_test))\n",
        "    }\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred_test).tolist()  # convert to list for JSON\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba_test)\n",
        "    roc_auc = float(auc(fpr, tpr))\n",
        "    metrics[\"ROC-AUC\"] = roc_auc\n",
        "\n",
        "    return metrics, cm, (fpr.tolist(), tpr.tolist(), roc_auc)\n",
        "\n",
        "# 5. MODEL SETUP WITH PARAM GRIDS\n",
        "models = {\n",
        "    \"MultinomialNB\": (MultinomialNB(), {\"alpha\": [0.1, 0.5, 1.0, 2.0]}),\n",
        "    \"ComplementNB\": (ComplementNB(), {\"alpha\": [0.1, 0.5, 1.0, 2.0]}),\n",
        "    \"BernoulliNB\": (BernoulliNB(), {\"alpha\": [0.1, 0.5, 1.0, 2.0], \"binarize\": [0.0, 0.1, 0.5]}),\n",
        "    \"LogisticRegression\": (LogisticRegression(max_iter=1000, solver=\"liblinear\"), {\"C\": [0.1, 1, 10]})\n",
        "}\n",
        "\n",
        "#  HYPERPARAMETER TUNING & EVALUATION\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "all_metrics, all_cms, all_rocs = [], {}, {}\n",
        "best_models = {}\n",
        "\n",
        "for name, (model, params) in models.items():\n",
        "    print(f\"\\n Tuning {name}...\")\n",
        "\n",
        "    Xtr, Xte = (X_train_nb, X_test_nb) if \"NB\" in name else (X_train, X_test)\n",
        "    grid = GridSearchCV(model, param_grid=params, scoring=\"f1\", cv=cv, n_jobs=-1, verbose=1)\n",
        "    grid.fit(Xtr, y_train)\n",
        "    best_model = grid.best_estimator_\n",
        "    best_models[name] = best_model\n",
        "\n",
        "    print(f\" Best Params for {name}: {grid.best_params_}\")\n",
        "\n",
        "    metrics, cm, roc_data = evaluate_model(best_model, Xtr, Xte, y_train, y_test, name)\n",
        "    all_metrics.append(metrics)\n",
        "    all_cms[name] = cm\n",
        "    all_rocs[name] = roc_data\n",
        "\n",
        "metrics_df = pd.DataFrame(all_metrics)\n",
        "print(\"\\n=== Final Model Performance ===\")\n",
        "print(metrics_df)\n",
        "\n",
        "\n",
        "os.makedirs(\"saved_models\", exist_ok=True)\n",
        "\n",
        "# Save models as .pkl\n",
        "for name, model in best_models.items():\n",
        "    with open(f\"saved_models/{name}_best.pkl\", \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "# Save metrics as JSON\n",
        "metrics_json = {\n",
        "    \"all_metrics\": all_metrics,\n",
        "    \"confusion_matrices\": all_cms,\n",
        "    \"roc_data\": all_rocs\n",
        "}\n",
        "with open(\"saved_models/metrics.json\", \"w\") as f:\n",
        "    json.dump(metrics_json, f, indent=4)\n",
        "\n",
        "print(\"\\n All models and metrics saved successfully in 'saved_models/' folder.\")\n",
        "\n",
        "# VISUALIZATION\n",
        "plt.figure(figsize=(10, 7))\n",
        "for name, (fpr, tpr, roc_auc) in all_rocs.items():\n",
        "    plt.plot(fpr, tpr, label=f\"{name} (AUC={roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], \"k--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curves for Probabilistic Models\")\n",
        "plt.legend()\n",
        "plt.savefig(\"saved_models/roc_curves.png\")\n",
        "plt.show()\n",
        "\n",
        "for name, cm in all_cms.items():\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.title(f\"{name} Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.savefig(f\"saved_models/{name}_cm.png\")\n",
        "    plt.show()\n",
        "\n",
        "metrics_df.set_index(\"Model\", inplace=True)\n",
        "metrics_df.plot(kind=\"bar\", figsize=(10, 6))\n",
        "plt.title(\"Model Performance Comparison\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"saved_models/model_comparison.png\")\n",
        "plt.show()\n",
        "\n",
        "#  BEST MODEL SELECTION\n",
        "best_idx = metrics_df[\"F1-Score\"].idxmax()\n",
        "best_row = metrics_df.loc[best_idx]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" BEST MODEL BASED ON F1-SCORE\")\n",
        "print(\"=\"*60)\n",
        "print(best_row)\n",
        "print(f\"\\n Recommended Model: {best_idx}\")\n",
        "\n",
        "# Save best model name\n",
        "with open(\"saved_models/best_model_name.json\", \"w\") as f:\n",
        "    json.dump({\"best_model\": best_idx}, f, indent=4)\n"
      ],
      "metadata": {
        "id": "KScJkG18eTc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "AGr2fJ8QuOVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from scipy import sparse\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, precision_recall_curve, average_precision_score, f1_score, precision_score, recall_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.optimize import minimize_scalar\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# 1. Load Preprocessed Data\n",
        "print(\"\\n[1/8] Loading optimized preprocessed dataset...\")\n",
        "\n",
        "X_processed = sparse.load_npz(\"X_processed.npz\")\n",
        "with open(\"y_target.pkl\", \"rb\") as f:\n",
        "    y = pickle.load(f)\n",
        "\n",
        "print(f\"âœ“ Dataset: {X_processed.shape}\")\n",
        "print(f\"âœ“ Fraud ratio: {y.mean()*100:.4f}% ({y.sum()} frauds out of {len(y)})\")\n",
        "\n",
        "# Convert to dense for better SVM performance\n",
        "X_dense = X_processed.toarray()\n",
        "print(f\"âœ“ Converted to dense format: {X_dense.shape}\")\n",
        "\n",
        "\n",
        "# STRATIFIED TRAIN-TEST SPLIT\n",
        "print(\"\\n[2/8] Creating stratified split...\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_dense, y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\" Training: {X_train.shape} - {y_train.sum()} frauds ({y_train.mean()*100:.4f}%)\")\n",
        "print(f\" Testing:  {X_test.shape} - {y_test.sum()} frauds ({y_test.mean()*100:.4f}%)\")\n",
        "\n",
        "\n",
        "# CLASS WEIGHT CALCULATION\n",
        "\n",
        "print(\"\\n[3/8] Calculating optimal class weights...\")\n",
        "\n",
        "#  Standard balanced weights\n",
        "standard_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "standard_weight_dict = {0: float(standard_weights[0]), 1: float(standard_weights[1])}\n",
        "\n",
        "#  Custom weights\n",
        "cost_missed_fraud = 10\n",
        "cost_false_alarm = 1\n",
        "custom_weights = {\n",
        "    0: 1.0,\n",
        "    1: float((len(y_train) - y_train.sum()) / y_train.sum() * (cost_missed_fraud / cost_false_alarm))\n",
        "}\n",
        "\n",
        "#  Progressive weights\n",
        "progressive_weights = [\n",
        "    {0: 1.0, 1: 5.0},\n",
        "    {0: 1.0, 1: 10.0},\n",
        "    {0: 1.0, 1: 15.0},\n",
        "    {0: 1.0, 1: 20.0},\n",
        "]\n",
        "\n",
        "print(f\"  Standard balanced weight: 1:{standard_weight_dict[1]:.2f}\")\n",
        "print(f\"  Custom cost-based weight: 1:{custom_weights[1]:.2f}\")\n",
        "print(f\"  Testing progressive weights: {[f'1:{w[1]}' for w in progressive_weights]}\")\n",
        "\n",
        "\n",
        "# SVM WITH DIFFERENT CLASS WEIGHTS\n",
        "print(\"\\n[4/8] Training ensemble of SVMs with different class weights...\")\n",
        "\n",
        "svm_models = {}\n",
        "svm_predictions = {}\n",
        "\n",
        "for i, class_weight in enumerate(progressive_weights):\n",
        "    print(f\"\\n  Training SVM {i+1}/4 with weight ratio 1:{class_weight[1]}\")\n",
        "\n",
        "    svm = LinearSVC(\n",
        "        C=0.1,\n",
        "        class_weight=class_weight,\n",
        "        max_iter=10000,\n",
        "        random_state=42 + i,\n",
        "        dual=True,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    cv_predictions = cross_val_predict(\n",
        "        svm, X_train, y_train,\n",
        "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
        "        method='decision_function'\n",
        "    )\n",
        "\n",
        "    svm.fit(X_train, y_train)\n",
        "\n",
        "    svm_models[f'svm_weight_{int(class_weight[1])}'] = {\n",
        "        'model': svm,\n",
        "        'weight': class_weight,\n",
        "        'cv_predictions': cv_predictions\n",
        "    }\n",
        "\n",
        "\n",
        "    test_scores = svm.decision_function(X_test)\n",
        "    svm_predictions[f'svm_weight_{int(class_weight[1])}'] = test_scores\n",
        "\n",
        "\n",
        "    cv_f1 = f1_score(y_train, (cv_predictions > 0).astype(int))\n",
        "    print(f\"    CV F1-Score: {cv_f1:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\n[5/8] Finding optimal thresholds for each SVM...\")\n",
        "\n",
        "def f_beta_score(precision, recall, beta=2):\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    return (1 + beta*2) * (precision * recall) / (beta*2 * precision + recall)\n",
        "\n",
        "def find_optimal_threshold(y_true, scores, beta=2):\n",
        "    def objective(threshold):\n",
        "        y_pred = (scores > threshold).astype(int)\n",
        "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "        return -f_beta_score(precision, recall, beta)\n",
        "\n",
        "    result = minimize_scalar(\n",
        "        objective,\n",
        "        bounds=(scores.min(), scores.max()),\n",
        "        method='bounded'\n",
        "    )\n",
        "    return float(result.x)\n",
        "\n",
        "\n",
        "optimal_thresholds = {}\n",
        "for model_name, scores in svm_predictions.items():\n",
        "    optimal_threshold = find_optimal_threshold(y_test, scores, beta=2)\n",
        "    optimal_thresholds[model_name] = optimal_threshold\n",
        "    print(f\"  {model_name:.<20} Optimal threshold: {optimal_threshold:.4f}\")\n",
        "\n",
        "# 6. MODEL SELECTION & ENSEMBLE\n",
        "\n",
        "print(\"\\n[6/8] Selecting best performing SVM...\")\n",
        "\n",
        "# Evaluate each model with its optimal threshold\n",
        "model_performances = {}\n",
        "\n",
        "for model_name, scores in svm_predictions.items():\n",
        "    threshold = optimal_thresholds[model_name]\n",
        "    y_pred = (scores > threshold).astype(int)\n",
        "\n",
        "    precision = float(precision_score(y_test, y_pred))\n",
        "    recall = float(recall_score(y_test, y_pred))\n",
        "    f1 = float(f1_score(y_test, y_pred))\n",
        "    f2 = float(f_beta_score(precision, recall, beta=2))\n",
        "\n",
        "    model_performances[model_name] = {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'f2': f2,\n",
        "        'threshold': threshold\n",
        "    }\n",
        "\n",
        "\n",
        "best_model_name = max(model_performances.keys(),\n",
        "                     key=lambda x: model_performances[x]['f2'])\n",
        "best_model_info = model_performances[best_model_name]\n",
        "best_svm = svm_models[best_model_name]['model']\n",
        "best_threshold = best_model_info['threshold']\n",
        "\n",
        "print(f\"\\n BEST MODEL: {best_model_name}\")\n",
        "print(f\"   F2-Score:  {best_model_info['f2']:.4f}\")\n",
        "print(f\"   F1-Score:  {best_model_info['f1']:.4f}\")\n",
        "print(f\"   Precision: {best_model_info['precision']:.4f}\")\n",
        "print(f\"   Recall:    {best_model_info['recall']:.4f}\")\n",
        "print(f\"   Threshold: {best_threshold:.4f}\")\n",
        "\n",
        "\n",
        "# EVALUATION\n",
        "\n",
        "print(\"\\n[7/8] Final evaluation with confidence calibration...\")\n",
        "\n",
        "# Get final predictions\n",
        "final_scores = best_svm.decision_function(X_test)\n",
        "y_pred_final = (final_scores > best_threshold).astype(int)\n",
        "\n",
        "def scores_to_probabilities(scores):\n",
        "    probas = 1 / (1 + np.exp(-scores))\n",
        "    calibrated_probas = (probas - best_threshold) / (1 - 2 * best_threshold)\n",
        "    return np.clip(calibrated_probas, 0, 1)\n",
        "\n",
        "y_proba_final = scores_to_probabilities(final_scores)\n",
        "\n",
        "roc_auc = float(roc_auc_score(y_test, y_proba_final))\n",
        "pr_auc = float(average_precision_score(y_test, y_proba_final))\n",
        "cm = confusion_matrix(y_test, y_pred_final)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print(f\"\\n FINAL PERFORMANCE METRICS:\")\n",
        "print(f\"  ROC-AUC:           {roc_auc:.4f}\")\n",
        "print(f\"  PR-AUC:            {pr_auc:.4f}\")\n",
        "print(f\"  F1-Score:          {best_model_info['f1']:.4f}\")\n",
        "print(f\"  F2-Score:          {best_model_info['f2']:.4f}\")\n",
        "print(f\"  Precision:         {best_model_info['precision']:.4f}\")\n",
        "print(f\"  Recall:            {best_model_info['recall']:.4f}\")\n",
        "\n",
        "print(f\"\\n BUSINESS IMPACT:\")\n",
        "print(f\"  Frauds Caught:     {tp}/{tp+fn} ({tp/(tp+fn)*100:.1f}%)\")\n",
        "print(f\"  Frauds Missed:     {fn}\")\n",
        "print(f\"  False Alarms:      {fp} ({fp/(fp+tn)*100:.1f}% of legitimate)\")\n",
        "print(f\"  Correct Legitimate: {tn}\")\n",
        "\n",
        "print(f\"\\n CLASSIFICATION REPORT:\")\n",
        "print(classification_report(y_test, y_pred_final, target_names=['Legitimate', 'Fraud'], digits=4))\n",
        "\n",
        "\n",
        "# SAVE MODELS WITH COMPREHENSIVE METRICS\n",
        "\n",
        "print(\"\\n[8/8] Saving SVM models with comprehensive metrics...\")\n",
        "\n",
        "def save_svm_model_with_metrics(model, metrics, model_name, threshold):\n",
        "    \"\"\"Save SVM model and its metrics\"\"\"\n",
        "    # Save the model\n",
        "    model_filename = f\"models/{model_name.replace(' ', '_').replace('(', '').replace(')', '').lower()}_model.pkl\"\n",
        "    with open(model_filename, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    # Save metrics\n",
        "    metrics_filename = f\"models/{model_name.replace(' ', '_').replace('(', '').replace(')', '').lower()}_metrics.json\"\n",
        "    metrics_to_save = {\n",
        "        'model_name': model_name,\n",
        "        'model_type': \"LinearSVC\",\n",
        "        'optimal_threshold': float(threshold),\n",
        "        'performance_metrics': {\n",
        "            'precision': float(metrics['precision']),\n",
        "            'recall': float(metrics['recall']),\n",
        "            'f1_score': float(metrics['f1']),\n",
        "            'f2_score': float(metrics['f2']),\n",
        "            'auc_roc': float(roc_auc),\n",
        "            'auc_pr': float(pr_auc)\n",
        "        },\n",
        "        'confusion_matrix': {\n",
        "            'tn': int(tn),\n",
        "            'fp': int(fp),\n",
        "            'fn': int(fn),\n",
        "            'tp': int(tp)\n",
        "        },\n",
        "        'business_metrics': {\n",
        "            'fraud_detection_rate': float(tp/(tp+fn)*100) if (tp+fn) > 0 else 0.0,\n",
        "            'false_alarm_rate': float(fp/(fp+tn)*100) if (fp+tn) > 0 else 0.0,\n",
        "            'frauds_caught': int(tp),\n",
        "            'frauds_missed': int(fn),\n",
        "            'false_positives': int(fp)\n",
        "        },\n",
        "        'training_info': {\n",
        "            'train_samples': int(X_train.shape[0]),\n",
        "            'test_samples': int(X_test.shape[0]),\n",
        "            'fraud_ratio': float(y.mean()),\n",
        "            'class_weight': svm_models[model_name]['weight'] if model_name in svm_models else None\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(metrics_filename, 'w') as f:\n",
        "        json.dump(metrics_to_save, f, indent=2)\n",
        "\n",
        "    return model_filename, metrics_filename\n",
        "\n",
        "saved_svm_models = []\n",
        "\n",
        "best_model_file, best_metrics_file = save_svm_model_with_metrics(\n",
        "    best_svm, best_model_info, \"Optimized SVM\", best_threshold\n",
        ")\n",
        "saved_svm_models.append({\n",
        "    'model': 'Optimized SVM',\n",
        "    'model_file': best_model_file,\n",
        "    'metrics_file': best_metrics_file\n",
        "})\n",
        "print(f\"  âœ“ {best_model_file}\")\n",
        "print(f\"  âœ“ {best_metrics_file}\")\n",
        "\n",
        "# Save SVM models for comparison\n",
        "for model_name, performance in model_performances.items():\n",
        "    if model_name != best_model_name:  # Already saved the best one\n",
        "        model = svm_models[model_name]['model']\n",
        "        model_file, metrics_file = save_svm_model_with_metrics(\n",
        "            model, performance, f\"SVM {model_name}\", performance['threshold']\n",
        "        )\n",
        "        saved_svm_models.append({\n",
        "            'model': f\"SVM {model_name}\",\n",
        "            'model_file': model_file,\n",
        "            'metrics_file': metrics_file\n",
        "        })\n",
        "        print(f\"  âœ“ {model_file}\")\n",
        "        print(f\"  âœ“ {metrics_file}\")\n",
        "\n",
        "svm_package = {\n",
        "    'best_model': best_svm,\n",
        "    'optimal_threshold': float(best_threshold),\n",
        "    'probability_calibration': scores_to_probabilities,\n",
        "    'performance_metrics': best_model_info,\n",
        "    'all_models_performance': model_performances,\n",
        "    'confusion_matrix': {\n",
        "        'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)\n",
        "    },\n",
        "    'auc_scores': {\n",
        "        'roc_auc': float(roc_auc),\n",
        "        'pr_auc': float(pr_auc)\n",
        "    },\n",
        "    'training_info': {\n",
        "        'dataset_shape': [int(X_dense.shape[0]), int(X_dense.shape[1])],\n",
        "        'fraud_ratio': float(y.mean()),\n",
        "        'train_samples': int(X_train.shape[0]),\n",
        "        'test_samples': int(X_test.shape[0])\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"models/optimized_svm_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(svm_package, f)\n",
        "\n",
        "with open(\"models/svm_performance_summary.json\", \"w\") as f:\n",
        "    json.dump({\n",
        "        'best_model': best_model_name,\n",
        "        'performance_metrics': best_model_info,\n",
        "        'business_impact': {\n",
        "            'frauds_caught': f\"{tp}/{tp+fn} ({tp/(tp+fn)*100:.1f}%)\",\n",
        "            'frauds_missed': int(fn),\n",
        "            'false_alarms': int(fp),\n",
        "            'fraud_detection_rate': float(tp/(tp+fn)*100) if (tp+fn) > 0 else 0.0,\n",
        "            'false_alarm_rate': float(fp/(fp+tn)*100) if (fp+tn) > 0 else 0.0\n",
        "        },\n",
        "        'all_models_comparison': model_performances\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"  âœ“ models/optimized_svm_model.pkl\")\n",
        "print(f\"  âœ“ models/svm_performance_summary.json\")\n",
        "\n",
        "-\n",
        "print(\"SVM TRAINING COMPLETED SUCCESSFULLY! ðŸŽ‰\")\n",
        "print(f\"\\n FINAL SVM PERFORMANCE SUMMARY:\")\n",
        "print(f\"   F2-Score:  {best_model_info['f2']:.4f} (Recall-focused)\")\n",
        "print(f\"   F1-Score:  {best_model_info['f1']:.4f} (Balanced)\")\n",
        "print(f\"   Recall:    {best_model_info['recall']:.4f} (Frauds Caught)\")\n",
        "print(f\"   Precision: {best_model_info['precision']:.4f} (Accuracy when Flagging)\")\n",
        "print(f\"   ROC-AUC:   {roc_auc:.4f}\")\n",
        "print(f\"   PR-AUC:    {pr_auc:.4f}\")\n",
        "\n",
        "print(f\"\\n BUSINESS RESULTS:\")\n",
        "print(f\"   â€¢ Catch {best_model_info['recall']*100:.1f}% of fraudulent jobs\")\n",
        "print(f\"   â€¢ {best_model_info['precision']*100:.1f}% accuracy when flagging fraud\")\n",
        "print(f\"   â€¢ Only {fn} fraudulent jobs missed\")\n",
        "print(f\"   â€¢ {fp} legitimate jobs incorrectly flagged\")\n",
        "\n",
        "print(f\"\\n Models saved in 'models/' directory:\")\n",
        "for saved in saved_svm_models:\n",
        "    print(f\"  â€¢ {saved['model']}: {saved['model_file']}\")\n",
        "\n",
        "print(f\"\\n Performance files:\")\n",
        "print(f\"  â€¢ SVM metrics: models/svm_performance_summary.json\")\n",
        "print(f\"  â€¢ Best model: models/optimized_svm_model.pkl\")\n",
        "\n",
        "print(f\"\\n SVM model is ready for production deployment!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "7si4spvDegzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost + LightGBM + Random Forest with SMOTE"
      ],
      "metadata": {
        "id": "m3NFXsIfuS9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  XGBoost + LightGBM + Random Forest with SMOTE\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, roc_auc_score,\n",
        "    precision_recall_curve, average_precision_score, f1_score,\n",
        "    recall_score, precision_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"XGBoost + LightGBM + Random Forest\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create models directory\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# LOAD PREPROCESSED DATA\n",
        "print(\"\\n[1/7] Loading preprocessed data...\")\n",
        "\n",
        "X = sparse.load_npz(\"X_processed.npz\")\n",
        "with open(\"y_target.pkl\", \"rb\") as f:\n",
        "    y = pickle.load(f)\n",
        "\n",
        "print(f\"âœ“ Dataset loaded: {X.shape}\")\n",
        "print(f\"âœ“ Fraud ratio: {y.mean()*100:.2f}% ({y.sum()} frauds out of {len(y)} total)\")\n",
        "\n",
        "# Convert to dense\n",
        "X_dense = X.toarray() if X.shape[1] < 1000 else X\n",
        "\n",
        "# TRAIN-TEST SPLIT\n",
        "print(\"\\n[2/7] Creating stratified train-test split...\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_dense, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\" Train set: {X_train.shape[0]} samples ({y_train.sum()} frauds)\")\n",
        "print(f\" Test set:  {X_test.shape[0]} samples ({y_test.sum()} frauds)\")\n",
        "\n",
        "# SAMPLING STRATEGIES\n",
        "print(\"\\n[3/7] Applying SMOTE + Random Undersampling...\")\n",
        "\n",
        "fraud_count = y_train.sum()\n",
        "legit_count = len(y_train) - fraud_count\n",
        "ratio = fraud_count / legit_count\n",
        "\n",
        "print(f\"  Original imbalance ratio: {ratio:.4f}\")\n",
        "\n",
        "#  SMOTE to increase minority class\n",
        "smote = SMOTE(\n",
        "    sampling_strategy=0.3,\n",
        "    random_state=42,\n",
        "    k_neighbors=min(5, fraud_count - 1)\n",
        ")\n",
        "\n",
        "#  Random undersampling to reduce majority class\n",
        "rus = RandomUnderSampler(\n",
        "    sampling_strategy=0.5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create pipeline\n",
        "sampling_pipeline = ImbPipeline([\n",
        "    ('smote', smote),\n",
        "    ('undersample', rus)\n",
        "])\n",
        "\n",
        "X_resampled, y_resampled = sampling_pipeline.fit_resample(X_train, y_train)\n",
        "\n",
        "print(f\" After resampling: {X_resampled.shape[0]} samples\")\n",
        "print(f\"  Frauds: {y_resampled.sum()}\")\n",
        "print(f\"  Legit: {len(y_resampled) - y_resampled.sum()}\")\n",
        "print(f\"  New ratio: {y_resampled.mean():.4f}\")\n",
        "\n",
        "# MODEL TRAINING - ENSEMBLE OF 3 MODELS\n",
        "print(\"\\n[4/7] Training ensemble models...\")\n",
        "\n",
        "scale_pos_weight = (len(y_resampled) - y_resampled.sum()) / y_resampled.sum()\n",
        "\n",
        "#  XGBoost\n",
        "print(\"\\n  [Model 1/3] XGBoost...\")\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    random_state=42,\n",
        "    eval_metric='aucpr',\n",
        "    tree_method='hist'\n",
        ")\n",
        "\n",
        "xgb_model.fit(\n",
        "    X_resampled, y_resampled,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    verbose=False\n",
        ")\n",
        "print(\"   XGBoost trained\")\n",
        "\n",
        "#  LightGBM\n",
        "print(\"\\n  [Model 2/3] LightGBM...\")\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=31,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    random_state=42,\n",
        "    metric='auc',\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "lgb_model.fit(\n",
        "    X_resampled, y_resampled,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    callbacks=[lgb.early_stopping(50, verbose=False)]\n",
        ")\n",
        "print(\"   LightGBM trained\")\n",
        "\n",
        "#  Random Forest (Balanced)\n",
        "print(\"\\n  [Model 3/3] Random Forest...\")\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=15,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=4,\n",
        "    max_features='sqrt',\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "rf_model.fit(X_resampled, y_resampled)\n",
        "print(\"  âœ“ Random Forest trained\")\n",
        "\n",
        "# 5. ENSEMBLE PREDICTIONS\n",
        "print(\"\\n[5/7] Creating ensemble predictions...\")\n",
        "\n",
        "# Get probability predictions from each model\n",
        "y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
        "y_pred_proba_lgb = lgb_model.predict_proba(X_test)[:, 1]\n",
        "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "weights = [0.40, 0.40, 0.20]\n",
        "y_pred_proba_ensemble = (\n",
        "    weights[0] * y_pred_proba_xgb +\n",
        "    weights[1] * y_pred_proba_lgb +\n",
        "    weights[2] * y_pred_proba_rf\n",
        ")\n",
        "\n",
        "print(\"âœ“ Ensemble predictions created\")\n",
        "print(f\"  XGBoost weight:      {weights[0]}\")\n",
        "print(f\"  LightGBM weight:     {weights[1]}\")\n",
        "print(f\"  Random Forest weight: {weights[2]}\")\n",
        "\n",
        "#OPTIMAL THRESHOLD TUNING (MAXIMIZE F2-SCORE)\n",
        "print(\"\\n[6/7] Finding optimal classification threshold...\")\n",
        "\n",
        "# F2-score emphasizes recall\n",
        "def f2_score_custom(precision, recall):\n",
        "    beta = 2\n",
        "    return (1 + beta*2) * (precision * recall) / (beta*2 * precision + recall + 1e-10)\n",
        "\n",
        "thresholds = np.arange(0.1, 0.9, 0.01)\n",
        "f2_scores = []\n",
        "f1_scores = []\n",
        "recalls = []\n",
        "precisions = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred_temp = (y_pred_proba_ensemble >= threshold).astype(int)\n",
        "    prec = precision_score(y_test, y_pred_temp, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred_temp, zero_division=0)\n",
        "    precisions.append(prec)\n",
        "    recalls.append(rec)\n",
        "    f2_scores.append(f2_score_custom(prec, rec))\n",
        "    f1_scores.append(f1_score(y_test, y_pred_temp, zero_division=0))\n",
        "\n",
        "optimal_threshold_f2 = float(thresholds[np.argmax(f2_scores)])\n",
        "optimal_threshold_f1 = float(thresholds[np.argmax(f1_scores)])\n",
        "\n",
        "print(f\" Optimal threshold (F2-score): {optimal_threshold_f2:.3f}\")\n",
        "print(f\" Optimal threshold (F1-score): {optimal_threshold_f1:.3f}\")\n",
        "print(f\"  Using F2 threshold (prioritizes catching frauds)\")\n",
        "\n",
        "optimal_threshold = optimal_threshold_f2\n",
        "y_pred_final = (y_pred_proba_ensemble >= optimal_threshold).astype(int)\n",
        "\n",
        "# EVALUATION & RESULTS\n",
        "\n",
        "print(\"\\n[7/7] Model Evaluation\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calculate metrics\n",
        "roc_auc = float(roc_auc_score(y_test, y_pred_proba_ensemble))\n",
        "avg_precision = float(average_precision_score(y_test, y_pred_proba_ensemble))\n",
        "prec_final = float(precision_score(y_test, y_pred_final))\n",
        "rec_final = float(recall_score(y_test, y_pred_final))\n",
        "f1_final = float(f1_score(y_test, y_pred_final))\n",
        "f2_final_manual = float(f2_score_custom(prec_final, rec_final))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_final)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "fraud_detection_rate = float(tp / (tp + fn) * 100) if (tp + fn) > 0 else 0.0\n",
        "false_alarm_rate = float(fp / (fp + tn) * 100) if (fp + tn) > 0 else 0.0\n",
        "\n",
        "print(f\"\\n{'OVERALL METRICS':^80}\")\n",
        "print(\"-\"*80)\n",
        "print(f\"  ROC-AUC Score:              {roc_auc:.4f}\")\n",
        "print(f\"  Average Precision (PR-AUC): {avg_precision:.4f}\")\n",
        "print(f\"  F1-Score:                   {f1_final:.4f}\")\n",
        "print(f\"  F2-Score (Recall-focused):  {f2_final_manual:.4f}\")\n",
        "print(f\"  Optimal Threshold:          {optimal_threshold:.4f}\")\n",
        "\n",
        "print(f\"\\n{'BUSINESS METRICS':^80}\")\n",
        "print(\"-\"*80)\n",
        "print(f\"  Fraud Detection Rate (Recall):  {fraud_detection_rate:.2f}%\")\n",
        "print(f\"  False Alarm Rate:               {false_alarm_rate:.2f}%\")\n",
        "print(f\"  Frauds Caught:                  {tp} out of {tp + fn}\")\n",
        "print(f\"  Frauds Missed:                  {fn}\")\n",
        "print(f\"  False Positives:                {fp}\")\n",
        "\n",
        "# SAVE MODELS WITH COMPREHENSIVE METRICS\n",
        "print(\"\\n[8/7] Saving models with comprehensive metrics...\")\n",
        "\n",
        "def save_model_with_metrics(model, metrics, model_name, threshold, model_type):\n",
        "    \"\"\"Save model and its metrics\"\"\"\n",
        "    # Save the model\n",
        "    model_filename = f\"models/{model_name.replace(' ', '_').replace('(', '').replace(')', '').lower()}_model.pkl\"\n",
        "    with open(model_filename, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    # Save metrics\n",
        "    metrics_filename = f\"models/{model_name.replace(' ', '_').replace('(', '').replace(')', '').lower()}_metrics.json\"\n",
        "    metrics_to_save = {\n",
        "        'model_name': model_name,\n",
        "        'model_type': model_type,\n",
        "        'optimal_threshold': float(threshold),\n",
        "        'performance_metrics': {\n",
        "            'precision': float(metrics['precision']),\n",
        "            'recall': float(metrics['recall']),\n",
        "            'f1_score': float(metrics['f1']),\n",
        "            'f2_score': float(metrics['f2']),\n",
        "            'auc_roc': float(metrics['auc_roc']),\n",
        "            'auc_pr': float(metrics['auc_pr'])\n",
        "        },\n",
        "        'confusion_matrix': {\n",
        "            'tn': int(metrics['tn']),\n",
        "            'fp': int(metrics['fp']),\n",
        "            'fn': int(metrics['fn']),\n",
        "            'tp': int(metrics['tp'])\n",
        "        },\n",
        "        'business_metrics': {\n",
        "            'fraud_detection_rate': float(metrics['fraud_detection_rate']),\n",
        "            'false_alarm_rate': float(metrics['false_alarm_rate']),\n",
        "            'frauds_caught': int(metrics['tp']),\n",
        "            'frauds_missed': int(metrics['fn'])\n",
        "        },\n",
        "        'training_info': {\n",
        "            'train_samples': int(X_train.shape[0]),\n",
        "            'test_samples': int(X_test.shape[0]),\n",
        "            'fraud_ratio': float(y.mean()),\n",
        "            'resampled_ratio': float(y_resampled.mean())\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(metrics_filename, 'w') as f:\n",
        "        json.dump(metrics_to_save, f, indent=2)\n",
        "\n",
        "    return model_filename, metrics_filename\n",
        "\n",
        "# Save individual models with their metrics\n",
        "saved_models = []\n",
        "\n",
        "# XGBoost metrics\n",
        "xgb_metrics = {\n",
        "    'precision': float(precision_score(y_test, (y_pred_proba_xgb >= optimal_threshold).astype(int))),\n",
        "    'recall': float(recall_score(y_test, (y_pred_proba_xgb >= optimal_threshold).astype(int))),\n",
        "    'f1': float(f1_score(y_test, (y_pred_proba_xgb >= optimal_threshold).astype(int))),\n",
        "    'f2': float(f2_score_custom(\n",
        "        precision_score(y_test, (y_pred_proba_xgb >= optimal_threshold).astype(int)),\n",
        "        recall_score(y_test, (y_pred_proba_xgb >= optimal_threshold).astype(int))\n",
        "    )),\n",
        "    'auc_roc': float(roc_auc_score(y_test, y_pred_proba_xgb)),\n",
        "    'auc_pr': float(average_precision_score(y_test, y_pred_proba_xgb)),\n",
        "    'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp),\n",
        "    'fraud_detection_rate': float(fraud_detection_rate),\n",
        "    'false_alarm_rate': float(false_alarm_rate)\n",
        "}\n",
        "\n",
        "model_file, metrics_file = save_model_with_metrics(xgb_model, xgb_metrics, \"XGBoost Ensemble\", optimal_threshold, \"XGBoost\")\n",
        "saved_models.append({'model': 'XGBoost Ensemble', 'model_file': model_file, 'metrics_file': metrics_file})\n",
        "print(f\"   {model_file}\")\n",
        "print(f\"   {metrics_file}\")\n",
        "\n",
        "# LightGBM metrics\n",
        "lgb_metrics = {\n",
        "    'precision': float(precision_score(y_test, (y_pred_proba_lgb >= optimal_threshold).astype(int))),\n",
        "    'recall': float(recall_score(y_test, (y_pred_proba_lgb >= optimal_threshold).astype(int))),\n",
        "    'f1': float(f1_score(y_test, (y_pred_proba_lgb >= optimal_threshold).astype(int))),\n",
        "    'f2': float(f2_score_custom(\n",
        "        precision_score(y_test, (y_pred_proba_lgb >= optimal_threshold).astype(int)),\n",
        "        recall_score(y_test, (y_pred_proba_lgb >= optimal_threshold).astype(int))\n",
        "    )),\n",
        "    'auc_roc': float(roc_auc_score(y_test, y_pred_proba_lgb)),\n",
        "    'auc_pr': float(average_precision_score(y_test, y_pred_proba_lgb)),\n",
        "    'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp),\n",
        "    'fraud_detection_rate': float(fraud_detection_rate),\n",
        "    'false_alarm_rate': float(false_alarm_rate)\n",
        "}\n",
        "\n",
        "model_file, metrics_file = save_model_with_metrics(lgb_model, lgb_metrics, \"LightGBM Ensemble\", optimal_threshold, \"LightGBM\")\n",
        "saved_models.append({'model': 'LightGBM Ensemble', 'model_file': model_file, 'metrics_file': metrics_file})\n",
        "print(f\"  âœ“ {model_file}\")\n",
        "print(f\"  âœ“ {metrics_file}\")\n",
        "\n",
        "# Random Forest metrics\n",
        "rf_metrics = {\n",
        "    'precision': float(precision_score(y_test, (y_pred_proba_rf >= optimal_threshold).astype(int))),\n",
        "    'recall': float(recall_score(y_test, (y_pred_proba_rf >= optimal_threshold).astype(int))),\n",
        "    'f1': float(f1_score(y_test, (y_pred_proba_rf >= optimal_threshold).astype(int))),\n",
        "    'f2': float(f2_score_custom(\n",
        "        precision_score(y_test, (y_pred_proba_rf >= optimal_threshold).astype(int)),\n",
        "        recall_score(y_test, (y_pred_proba_rf >= optimal_threshold).astype(int))\n",
        "    )),\n",
        "    'auc_roc': float(roc_auc_score(y_test, y_pred_proba_rf)),\n",
        "    'auc_pr': float(average_precision_score(y_test, y_pred_proba_rf)),\n",
        "    'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp),\n",
        "    'fraud_detection_rate': float(fraud_detection_rate),\n",
        "    'false_alarm_rate': float(false_alarm_rate)\n",
        "}\n",
        "\n",
        "model_file, metrics_file = save_model_with_metrics(rf_model, rf_metrics, \"Random Forest Ensemble\", optimal_threshold, \"Random Forest\")\n",
        "saved_models.append({'model': 'Random Forest Ensemble', 'model_file': model_file, 'metrics_file': metrics_file})\n",
        "print(f\"  âœ“ {model_file}\")\n",
        "print(f\"  âœ“ {metrics_file}\")\n",
        "\n",
        "# Save ensemble configuration with comprehensive metrics\n",
        "ensemble_metrics = {\n",
        "    'precision': float(prec_final),\n",
        "    'recall': float(rec_final),\n",
        "    'f1_score': float(f1_final),\n",
        "    'f2_score': float(f2_final_manual),\n",
        "    'auc_roc': float(roc_auc),\n",
        "    'auc_pr': float(avg_precision),\n",
        "    'optimal_threshold': float(optimal_threshold),\n",
        "    'fraud_detection_rate': float(fraud_detection_rate),\n",
        "    'false_alarm_rate': float(false_alarm_rate)\n",
        "}\n",
        "\n",
        "ensemble_config = {\n",
        "    'weights': [float(w) for w in weights],\n",
        "    'optimal_threshold': float(optimal_threshold),\n",
        "    'sampling_pipeline': sampling_pipeline,\n",
        "    'metrics': ensemble_metrics,\n",
        "    'model_files': [\n",
        "        {\n",
        "            'model': saved['model'],\n",
        "            'model_file': saved['model_file'],\n",
        "            'metrics_file': saved['metrics_file']\n",
        "        } for saved in saved_models\n",
        "    ],\n",
        "    'training_info': {\n",
        "        'dataset_shape': [int(X.shape[0]), int(X.shape[1])],\n",
        "        'fraud_ratio': float(y.mean()),\n",
        "        'train_samples': int(X_train.shape[0]),\n",
        "        'test_samples': int(X_test.shape[0]),\n",
        "        'resampled_samples': int(X_resampled.shape[0]),\n",
        "        'resampled_ratio': float(y_resampled.mean())\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"models/ensemble_config.pkl\", \"wb\") as f:\n",
        "    pickle.dump(ensemble_config, f)\n",
        "\n",
        "# Save ensemble metrics as JSON for easy reading\n",
        "with open(\"models/ensemble_metrics.json\", \"w\") as f:\n",
        "    json.dump({\n",
        "        'ensemble_performance': ensemble_metrics,\n",
        "        'model_weights': [float(w) for w in weights],\n",
        "        'optimal_threshold': float(optimal_threshold),\n",
        "        'business_impact': {\n",
        "            'frauds_caught': f\"{tp}/{tp+fn} ({tp/(tp+fn)*100:.1f}%)\",\n",
        "            'frauds_missed': int(fn),\n",
        "            'false_alarms': int(fp),\n",
        "            'fraud_detection_rate': float(fraud_detection_rate),\n",
        "            'false_alarm_rate': float(false_alarm_rate)\n",
        "        }\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"  âœ“ models/ensemble_config.pkl\")\n",
        "print(f\"  âœ“ models/ensemble_metrics.json\")\n",
        "\n",
        "# Save comprehensive results summary\n",
        "summary_results = {\n",
        "    'training_parameters': {\n",
        "        'dataset_shape': [int(X.shape[0]), int(X.shape[1])],\n",
        "        'fraud_ratio': float(y.mean()),\n",
        "        'train_samples': int(X_train.shape[0]),\n",
        "        'test_samples': int(X_test.shape[0]),\n",
        "        'resampled_ratio': float(y_resampled.mean())\n",
        "    },\n",
        "    'ensemble_performance': ensemble_metrics,\n",
        "    'individual_models': {\n",
        "        'XGBoost': {\n",
        "            'precision': float(xgb_metrics['precision']),\n",
        "            'recall': float(xgb_metrics['recall']),\n",
        "            'f1_score': float(xgb_metrics['f1']),\n",
        "            'f2_score': float(xgb_metrics['f2']),\n",
        "            'auc_roc': float(xgb_metrics['auc_roc']),\n",
        "            'auc_pr': float(xgb_metrics['auc_pr'])\n",
        "        },\n",
        "        'LightGBM': {\n",
        "            'precision': float(lgb_metrics['precision']),\n",
        "            'recall': float(lgb_metrics['recall']),\n",
        "            'f1_score': float(lgb_metrics['f1']),\n",
        "            'f2_score': float(lgb_metrics['f2']),\n",
        "            'auc_roc': float(lgb_metrics['auc_roc']),\n",
        "            'auc_pr': float(lgb_metrics['auc_pr'])\n",
        "        },\n",
        "        'RandomForest': {\n",
        "            'precision': float(rf_metrics['precision']),\n",
        "            'recall': float(rf_metrics['recall']),\n",
        "            'f1_score': float(rf_metrics['f1']),\n",
        "            'f2_score': float(rf_metrics['f2']),\n",
        "            'auc_roc': float(rf_metrics['auc_roc']),\n",
        "            'auc_pr': float(rf_metrics['auc_pr'])\n",
        "        }\n",
        "    },\n",
        "    'saved_files': [\n",
        "        {\n",
        "            'model': saved['model'],\n",
        "            'model_file': saved['model_file'],\n",
        "            'metrics_file': saved['metrics_file']\n",
        "        } for saved in saved_models\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open('models/ensemble_summary.json', 'w') as f:\n",
        "    json.dump(summary_results, f, indent=2)\n",
        "\n",
        "print(f\"  âœ“ models/ensemble_summary.json\")\n",
        "\n",
        "# SUMMARY\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ENSEMBLE MODEL TRAINING COMPLETED! \")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nâœ“ Ensemble model achieves {fraud_detection_rate:.1f}% fraud detection rate\")\n",
        "print(f\"âœ“ ROC-AUC: {roc_auc:.3f} | PR-AUC: {avg_precision:.3f} | F2-Score: {f2_final_manual:.3f}\")\n",
        "print(f\"âœ“ Optimal threshold: {optimal_threshold:.3f}\")\n",
        "print(f\"âœ“ Catches {tp} out of {tp + fn} frauds with {fp} false alarms\")\n",
        "\n",
        "print(f\"\\n Models saved in 'models/' directory:\")\n",
        "for saved in saved_models:\n",
        "    print(f\"  â€¢ {saved['model']}: {saved['model_file']}\")\n",
        "\n",
        "print(f\"\\nPerformance files:\")\n",
        "print(f\"  â€¢ Ensemble metrics: models/ensemble_metrics.json\")\n",
        "print(f\"  â€¢ Ensemble summary: models/ensemble_summary.json\")\n",
        "print(f\"  â€¢ Ensemble config: models/ensemble_config.pkl\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zxeiS318gDRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LightBGM with weights and smote"
      ],
      "metadata": {
        "id": "Scs8IGmwugE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LightBGM both - weights and smote\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (confusion_matrix, precision_recall_curve,\n",
        "                            f1_score, roc_auc_score, average_precision_score)\n",
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "import lightgbm as lgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# Load preprocessed data\n",
        "print(\"\\n[STEP 1] Loading preprocessed data...\")\n",
        "X = sparse.load_npz(\"X_processed.npz\")\n",
        "with open(\"y_target.pkl\", \"rb\") as f:\n",
        "    y = pickle.load(f)\n",
        "\n",
        "print(f\" Dataset shape: {X.shape}\")\n",
        "print(f\" Fraud ratio: {y.mean()*100:.2f}%\")\n",
        "print(f\" Fraud samples: {y.sum()}\")\n",
        "\n",
        "# Calculate weights\n",
        "scale_pos_weight = (len(y) - y.sum()) / y.sum()\n",
        "print(f\" scale_pos_weight: {scale_pos_weight:.2f}\")\n",
        "\n",
        "\n",
        "#  Train-Test Split\n",
        "\n",
        "print(\"\\n[STEP 2] Creating stratified train-test split...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(f\" Train: {X_train.shape[0]} samples ({y_train.sum()} fraud)\")\n",
        "print(f\" Test:  {X_test.shape[0]} samples ({y_test.sum()} fraud)\")\n",
        "\n",
        "\n",
        "#  Utility Functions\n",
        "def print_metrics(y_true, y_pred, y_pred_proba, model_name=\"Model\"):\n",
        "    \"\"\"Print comprehensive metrics\"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"{model_name} - RESULTS\")\n",
        "    print('='*50)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    print(f\"Confusion Matrix:\")\n",
        "    print(f\"TN: {tn:5d} | FP: {fp:5d}\")\n",
        "    print(f\"FN: {fn:5d} | TP: {tp:5d}\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "    auc_roc = roc_auc_score(y_true, y_pred_proba)\n",
        "    auc_pr = average_precision_score(y_true, y_pred_proba)\n",
        "\n",
        "    print(f\"\\nPrecision:    {precision:.4f}\")\n",
        "    print(f\"Recall:       {recall:.4f}\")\n",
        "    print(f\"F1-Score:     {f1:.4f}\")\n",
        "    print(f\"Specificity:  {specificity:.4f}\")\n",
        "    print(f\"AUC-ROC:      {auc_roc:.4f}\")\n",
        "    print(f\"AUC-PR:       {auc_pr:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1': float(f1),\n",
        "        'specificity': float(specificity),\n",
        "        'auc_roc': float(auc_roc),\n",
        "        'auc_pr': float(auc_pr),\n",
        "        'confusion_matrix': {\n",
        "            'tn': int(tn),\n",
        "            'fp': int(fp),\n",
        "            'fn': int(fn),\n",
        "            'tp': int(tp)\n",
        "        },\n",
        "        'model': model_name\n",
        "    }\n",
        "\n",
        "def optimize_threshold(y_true, y_pred_proba):\n",
        "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred_proba)\n",
        "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
        "    optimal_idx = np.argmax(f1_scores)\n",
        "    optimal_threshold = float(thresholds[optimal_idx]) if optimal_idx < len(thresholds) else 0.5\n",
        "    return optimal_threshold, float(f1_scores[optimal_idx])\n",
        "\n",
        "def save_model_with_metrics(model, metrics, model_name, threshold):\n",
        "    os.makedirs('models', exist_ok=True)\n",
        "\n",
        "    # Save the model\n",
        "    model_filename = f\"models/{model_name.replace(' ', '_').replace('(', '').replace(')', '').lower()}_model.pkl\"\n",
        "    with open(model_filename, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    # Save metrics\n",
        "    metrics_filename = f\"models/{model_name.replace(' ', '_').replace('(', '').replace(')', '').lower()}_metrics.json\"\n",
        "    metrics_to_save = {\n",
        "        'model_name': model_name,\n",
        "        'optimal_threshold': float(threshold),\n",
        "        'performance_metrics': {\n",
        "            'precision': metrics['precision'],\n",
        "            'recall': metrics['recall'],\n",
        "            'f1_score': metrics['f1'],\n",
        "            'specificity': metrics['specificity'],\n",
        "            'auc_roc': metrics['auc_roc'],\n",
        "            'auc_pr': metrics['auc_pr']\n",
        "        },\n",
        "        'confusion_matrix': metrics['confusion_matrix'],\n",
        "        'training_info': {\n",
        "            'train_samples': int(X_train.shape[0]),\n",
        "            'test_samples': int(X_test.shape[0]),\n",
        "            'fraud_ratio': float(y.mean()),\n",
        "            'scale_pos_weight': float(scale_pos_weight)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(metrics_filename, 'w') as f:\n",
        "        json.dump(metrics_to_save, f, indent=2)\n",
        "\n",
        "    return model_filename, metrics_filename\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "saved_models = []\n",
        "\n",
        "\n",
        "print(\"TRAINING OPTIMIZED MODELS\")\n",
        "# LightGBM with Custom Configuration\n",
        "\n",
        "print(\"\\n[MODEL 1/2] LightGBM with Custom Configuration...\")\n",
        "\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    max_depth=7,\n",
        "    learning_rate=0.01,\n",
        "    n_estimators=500,\n",
        "    num_leaves=31,\n",
        "    min_child_samples=20,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=1.0,\n",
        "    random_state=42,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "lgb_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_proba = lgb_model.predict_proba(X_test)[:, 1]\n",
        "optimal_thresh, optimal_f1 = optimize_threshold(y_test, y_pred_proba)\n",
        "print(f\" Optimal threshold: {optimal_thresh:.4f}\")\n",
        "\n",
        "y_pred = (y_pred_proba >= optimal_thresh).astype(int)\n",
        "result = print_metrics(y_test, y_pred, y_pred_proba, \"LightGBM (Optimized)\")\n",
        "results.append(result)\n",
        "\n",
        "# Save LightGBM model and metrics\n",
        "model_file, metrics_file = save_model_with_metrics(lgb_model, result, \"LightGBM Optimized\", optimal_thresh)\n",
        "saved_models.append({'model': 'LightGBM Optimized', 'model_file': model_file, 'metrics_file': metrics_file})\n",
        "print(f\" Model saved: {model_file}\")\n",
        "print(f\" Metrics saved: {metrics_file}\")\n",
        "\n",
        "# Borderline SMOTE + LightGBM\n",
        "print(\"\\n[MODEL 2/2] Borderline-SMOTE + LightGBM...\")\n",
        "\n",
        "borderline_smote = BorderlineSMOTE(sampling_strategy=0.5, random_state=42, k_neighbors=5)\n",
        "\n",
        "X_train_bsmote, y_train_bsmote = borderline_smote.fit_resample(X_train, y_train)\n",
        "print(f\"After Borderline-SMOTE: {y_train_bsmote.sum()} fraud samples\")\n",
        "\n",
        "lgb_bsmote = lgb.LGBMClassifier(\n",
        "    scale_pos_weight=2.0,\n",
        "    max_depth=7,\n",
        "    learning_rate=0.01,\n",
        "    n_estimators=400,\n",
        "    num_leaves=31,\n",
        "    random_state=42,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "lgb_bsmote.fit(X_train_bsmote, y_train_bsmote)\n",
        "\n",
        "y_pred_proba = lgb_bsmote.predict_proba(X_test)[:, 1]\n",
        "optimal_thresh, optimal_f1 = optimize_threshold(y_test, y_pred_proba)\n",
        "print(f\"Optimal threshold: {optimal_thresh:.4f}\")\n",
        "\n",
        "y_pred = (y_pred_proba >= optimal_thresh).astype(int)\n",
        "result = print_metrics(y_test, y_pred, y_pred_proba, \"Borderline-SMOTE + LightGBM\")\n",
        "results.append(result)\n",
        "\n",
        "# Save Borderline-SMOTE + LightGBM model and metrics\n",
        "model_file, metrics_file = save_model_with_metrics(lgb_bsmote, result, \"Borderline SMOTE LightGBM\", optimal_thresh)\n",
        "saved_models.append({'model': 'Borderline SMOTE LightGBM', 'model_file': model_file, 'metrics_file': metrics_file})\n",
        "print(f\" Model saved: {model_file}\")\n",
        "print(f\" Metrics saved: {metrics_file}\")\n",
        "\n",
        "# FINAL COMPARISON AND SAVE ALL RESULTS\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values('f1', ascending=False)\n",
        "\n",
        "print(f\"\\n{'Model':<30}{'F1':<10}{'Precision':<12}{'Recall':<10}{'AUC-PR':<10}\")\n",
        "print(\"-\"*60)\n",
        "for idx, row in results_df.iterrows():\n",
        "    print(f\"{row['model']:<30}{row['f1']:<10.4f}{row['precision']:<12.4f}{row['recall']:<10.4f}{row['auc_pr']:<10.4f}\")\n",
        "\n",
        "# Save comprehensive results summary\n",
        "summary_results = {\n",
        "    'training_parameters': {\n",
        "        'dataset_shape': [int(X.shape[0]), int(X.shape[1])],\n",
        "        'fraud_ratio': float(y.mean()),\n",
        "        'train_samples': int(X_train.shape[0]),\n",
        "        'test_samples': int(X_test.shape[0]),\n",
        "        'scale_pos_weight': float(scale_pos_weight)\n",
        "    },\n",
        "    'model_performance': [],\n",
        "    'saved_files': saved_models\n",
        "}\n",
        "\n",
        "for result in results:\n",
        "    summary_results['model_performance'].append({\n",
        "        'model_name': result['model'],\n",
        "        'f1_score': result['f1'],\n",
        "        'precision': result['precision'],\n",
        "        'recall': result['recall'],\n",
        "        'specificity': result['specificity'],\n",
        "        'auc_roc': result['auc_roc'],\n",
        "        'auc_pr': result['auc_pr'],\n",
        "        'confusion_matrix': result['confusion_matrix']\n",
        "    })\n",
        "\n",
        "with open('models/all_models_summary.json', 'w') as f:\n",
        "    json.dump(summary_results, f, indent=2)\n",
        "\n",
        "print(f\"Comprehensive results summary saved: models/all_models_summary.json\")\n",
        "\n",
        "# Identify top model\n",
        "best_model = results_df.iloc[0]\n",
        "print(f\"Model: {best_model['model']}\")\n",
        "print(f\"F1 Score: {best_model['f1']:.4f}\")\n",
        "print(f\"Precision: {best_model['precision']:.4f}\")\n",
        "print(f\"Recall: {best_model['recall']:.4f}\")\n",
        "print(f\"AUC-PR: {best_model['auc_pr']:.4f}\")\n",
        "\n",
        "if best_model['f1'] >= 0.75:\n",
        "    print(\"TARGET ACHIEVED!\")\n",
        "else:\n",
        "    print(f\" Gap to target: {0.75 - best_model['f1']:.4f}\")\n",
        "\n",
        "# Print saved files summary\n",
        "\n",
        "for saved in saved_models:\n",
        "    print(f\" {saved['model']}:\")\n",
        "    print(f\"  - Model: {saved['model_file']}\")\n",
        "    print(f\"  - Metrics: {saved['metrics_file']}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pMvjZr8kQyoY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}